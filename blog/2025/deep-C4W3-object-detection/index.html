<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Object Detection (Deep Learning Notes C4W3) | Colin Young's Personal Site </title> <meta name="author" content="Colin Young"> <meta name="description" content="object detection and the YOLO algorithm"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-object-detection/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Colin Young's Personal Site </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">logs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Object Detection (Deep Learning Notes C4W3)</h1> <p class="post-meta"> Created on December 30, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/cnn"> <i class="fa-solid fa-hashtag fa-sm"></i> CNN</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/computer-science"> <i class="fa-solid fa-tag fa-sm"></i> computer-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks" rel="external nofollow noopener" target="_blank">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning" rel="external nofollow noopener" target="_blank">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 3.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <h2 id="typical-tasks-in-computer-vision">Typical Tasks in Computer Vision</h2> <ul> <li> <p><strong>Image Classification</strong>: classify an image to a specific class (usually only one object in the whole image)</p> </li> <li> <p><strong>Classification with Localisation</strong>: learn the class of the object, and generate a bounding box to give the location of the object in the image (usually only one object in the whole image)</p> </li> <li> <p><strong>Object Detection</strong>: detect all the objects in the image and predict their classes and give their locations (usually more than one object from different classes in the image)</p> </li> <li> <p><strong>Semantic Segmentation</strong>: label each pixel in the image with a category label</p> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_image_segmentation-480.webp 480w,/assets/img/deep/C4W3_image_segmentation-800.webp 800w,/assets/img/deep/C4W3_image_segmentation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_image_segmentation.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Common Computer Vision Tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Common Computer Vision Tasks </div> <h2 id="object-detection">Object Detection</h2> <h4 id="defining-the-outputs">Defining the Outputs</h4> <p>The output vector \(y\) in the classification with localisation problem can be defined as: \(y = \left[ \begin{array}{c} P_c \\ b_x \\ b_y \\ b_w \\ b_h \\ C_1 \\ C_2 \\ \vdots \\ C_s \end{array} \right]\) where</p> <ul> <li>\(P_c\) gives the probability if there is an object in the image</li> <li>\(b_x\) and \(b_y\) give the coordinates of the centre of the object</li> <li>\(b_w\) and \(b_h\) are the width and height of the object</li> <li>\(C_1, C_2, \cdots , C_s\) are the probabilities that the object belongs to each of the \(s\)​ classes</li> </ul> <p>The output \(y\) can also be defined to be: \(y = \left[ \begin{array}{c} P_c \\ b_x \\ b_y \\ b_w \\ b_h \\ c \end{array} \right]\) where \(c\) is the class index that represents one of the \(s\) classes to which the object belongs.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_box_label-480.webp 480w,/assets/img/deep/C4W3_box_label-800.webp 800w,/assets/img/deep/C4W3_box_label-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_box_label.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Output Vector" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Output Vector for the Problem </div> <h4 id="landmarks">Landmarks</h4> <p>In computer vision problems like face recognition problems, we might also want to output some points on the face like corners of the eyes, corners of he mouth, corners of the nose and so on, which makes it possible to predict the facial expressions of that person. Another example is when we get the skeleton of a person, outputting the positions of the hands, elbows, shoulders, knees, feet and so on could be helpful in predicting what the person is doing. This is called <strong>landmark detection</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_Facial%20Landmark%20Detection-480.webp 480w,/assets/img/deep/C4W3_Facial%20Landmark%20Detection-800.webp 800w,/assets/img/deep/C4W3_Facial%20Landmark%20Detection-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_Facial%20Landmark%20Detection.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Facial Landmarks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Facial Landmarks </div> <p>Instead of using four numbers to give the location and the size of the object of interest, a collection of landmarks are needed in the labelled data. \(y=\left[ \begin{array}{c} P_c \\ l_{1,x} \\ l_{1,y} \\ l_{2,x} \\ l_{2,y} \\ \vdots \\ l_{s,x} \\ l_{s,y} \end{array} \right]\) where \((l_{i,x}, l_{i,y})\) are the coordinates of the \(i^\text{th}\) landmark which can be the left corner of the left eye, or the right corner of the nose, etc.</p> <h4 id="choosing-the-loss-function">Choosing the Loss Function</h4> <p>There are two primary tasks in an object detection problem: <strong>classification</strong> (identifying the class of the object) and <strong>localisation</strong> (specifying the object’s position and size). In practice, we use a combination of loss functions.</p> <p>In the course, Andrew’s recommendations are:</p> <ul> <li>logistic regression for \(P_c\) and log-likelihood loss for the classes (standard loss function for most classification tasks)</li> <li>mean squared error for the bounding boxes</li> </ul> <h4 id="sliding-window-technique">Sliding Window Technique</h4> <p>For object detection problems, there could be many objects scattered at multiple places across an image, but we can build upon what we had learned earlier in this course and use the sliding window technique.</p> <p>Recall that we have learned how to train a CNN for the image classification problems, that is to identity the only object in one image. For object detection, we can crop specific windows of the images (with varying sizes and varying ratios) and forward the cropped portion into a CNN and predict the corresponding class for each window.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_sliding-window-in-action-480.webp 480w,/assets/img/deep/C4W3_sliding-window-in-action-800.webp 800w,/assets/img/deep/C4W3_sliding-window-in-action-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_sliding-window-in-action.gif" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Sliding Windows" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Sliding Window Technique </div> <p>The disadvantage of this method is its high computational cost. It turns out that the sliding windows can be implemented more efficiently using convolutional networks.</p> <h4 id="sliding-window-with-convolutional-implementation">Sliding Window with Convolutional Implementation</h4> <p>The illustration below shows how a convolutional network takes a \(16\times16\times3\) image and produces a \(2\times2\times4\) output. Each \(1\times1\times4\) slice in the output makes a prediction about the probability that each corresponding sliding window (as shown in different colours) belongs to each one of the \(4\) classes. Similar architectures can be applied for bigger images and more classes.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_sliding-window-by-convolution-480.webp 480w,/assets/img/deep/C4W3_sliding-window-by-convolution-800.webp 800w,/assets/img/deep/C4W3_sliding-window-by-convolution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_sliding-window-by-convolution.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Sliding Window with Convolutional Implementation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Sliding Window with Convolutional Implementation </div> <p>The sliding window approach treats each window as an independent image patch separately, but neighbouring sliding windows with large fractions of overlapping may share a lot of repeated computations. Therefore, processing the entire image as a whole with a CNN, which shares and reuses the feature map for multiple regions, can significantly eliminate redundant computations and hence reduce computational cost.</p> <h2 id="yolo">YOLO</h2> <p><strong>YOLO</strong> (<em>You Only Look Once</em>) is a popular algorithm for object detection problems because of its high accuracy and also its ability to run in real time. This algorithm was named YOLO as it requires only one forward propagation pass through the network to make predictions, so in this sense it only looks once at the image. After non-max suppression, it then outputs recognized objects together with the bounding boxes.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_yolo_on_predict_time-480.webp 480w,/assets/img/deep/C4W3_yolo_on_predict_time-800.webp 800w,/assets/img/deep/C4W3_yolo_on_predict_time-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_yolo_on_predict_time.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="YOLO in Real Time" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> YOLO in Real Time </div> <h4 id="bounding-box-prediction">Bounding Box Prediction</h4> <p>By dividing the input image into a smaller grid of size \(G \times G\), we can perform a simple object localization for each grid cell where the network outputs the class probabilities and the bounding boxes of the main object in that grid cell. For each grid cell, the network detects the object for which its centre belongs to that grid cell, even if the object may span into neighbouring grid cells.</p> <p>For each grid cell, if we define the target label as mentioned earlier as</p> \[y = \left[ \begin{array}{c} P_c \\ b_x \\ b_y \\ b_w \\ b_h \\ C_1 \\ C_2 \\ \vdots \\ C_s \end{array} \right]\] <p>then the dimension of the output layer is \(G \times G \times (s + 5)\).</p> <h4 id="anchor-boxes">Anchor Boxes</h4> <p>Objects from different classes usually have different shapes. For example, we may expect the bounding box for a pedestrian to be tall and thin, while the bounding box for a car to be relatively wider. To represent different objects in the training data, we choose reasonable height/width ratios for different classes. Such bounding boxes with predefined reference shapes are called <strong>anchor boxes</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_anchor-boxes-480.webp 480w,/assets/img/deep/C4W3_anchor-boxes-800.webp 800w,/assets/img/deep/C4W3_anchor-boxes-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_anchor-boxes.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Anchor Boxes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Anchor Boxes </div> <p>Suppose we have \(n_A\) anchor boxes, then the dimension of the output tensor of the last layer is \(G \times G \times n_A \times (s+5)\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_architecture-480.webp 480w,/assets/img/deep/C4W3_architecture-800.webp 800w,/assets/img/deep/C4W3_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_architecture.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Encoding for Multiple Anchor Boxes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Encoding for Multiple Anchor Boxes </div> <h4 id="class-scores">Class Scores</h4> <p>We can further extract a probability that the box contains a certain class by computing the element-wise product \(P_c \times C_i\). After calculating the score for all \(s\) classes in one anchor box, we may identify the maximum score and assign to this anchor box this class score and the corresponding class.</p> <h4 id="non-max-suppression">Non-Max Suppression</h4> <p>One problem with YOLO at this stage is that it detects the same object multiple times. The model would output \(G \times G \times n_A\) boxes, which are way too many boxes. We need to reduce the algorithm’s output to a much smaller number of detected objects. To do so, we can use non-max suppression to make sure that YOLO detect each object just once. This involves the following steps:</p> <ul> <li>choose a threshold for the class scores and get rid of the boxes with a low class score</li> <li>select only one box when several boxes overlap with each other but detect the same object</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_non-max-suppression-480.webp 480w,/assets/img/deep/C4W3_non-max-suppression-800.webp 800w,/assets/img/deep/C4W3_non-max-suppression-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_non-max-suppression.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Non-Max Suppression" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Non-Max Suppression </div> <p>To determine which boxes detect the same object, one measure that can be used is the <strong>intersection over union</strong>. The <em>IoU</em> between two bounding boxes \(B_1\) and \(B_2\) is defined as:</p> \[IoU = \frac{B_1 \cap B_2}{B_1 \cup B_2}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_IoU-480.webp 480w,/assets/img/deep/C4W3_IoU-800.webp 800w,/assets/img/deep/C4W3_IoU-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_IoU.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="IoU" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> IoU (Intersection over Union) </div> <p>While there are too many remaining boxes, we pick the box with the largest \(P_c\) and output that as a prediction. At the mean time, we search for and remove any remaining box with the same output but with an <em>IoU</em> that is greater than a certain threshold.</p> <p>If there are \(s\) classes to be detected, we should run non-max suppression \(s\) times, once for each output class.</p> <h4 id="visualising-yolo">Visualising YOLO</h4> <p>One way to visualize YOLO’s output is to plot the bounding boxes that it predicts. Doing this results in a picture like this:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_Detected-with-YOLO-480.webp 480w,/assets/img/deep/C4W3_Detected-with-YOLO-800.webp 800w,/assets/img/deep/C4W3_Detected-with-YOLO-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_Detected-with-YOLO.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Objects Detected with YOLO" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Objects Detected with YOLO </div> <p>Another way to visualise what YOLO predicts on an image is to do the following:</p> <ul> <li>for each grid cell, find the highest probability score (take a maximum score across the \(s\) classes, and one maximum for each of the \(n_A\) anchor boxes)</li> <li>colour that grid cell according to what object that grid cell considers the most likely</li> </ul> <p>Doing this results in a picture that looks like this:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_proba_map-480.webp 480w,/assets/img/deep/C4W3_proba_map-800.webp 800w,/assets/img/deep/C4W3_proba_map-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_proba_map.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Colouring Labels with YOLO" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Colouring Labels with YOLO </div> </div> </article> </div> <style>.img-natural{max-width:90%;width:auto!important;height:auto;display:block;margin-left:auto;margin-right:auto}h2{color:var(--global-bg-color);background:var(--global-theme-color);line-height:1.6em;border-radius:3px;padding:2px;display:inline-block}</style> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Colin Young. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>