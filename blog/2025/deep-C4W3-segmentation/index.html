<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Image Semantic Segmentation (Deep Learning Notes C4W3) | Colin Young's Personal Site </title> <meta name="author" content="Colin Young"> <meta name="description" content="labelling and colouring the pixels of an image into a set of predefined classes"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-segmentation/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Colin Young's Personal Site </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">logs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Image Semantic Segmentation (Deep Learning Notes C4W3)</h1> <p class="post-meta"> Created on December 30, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/cnn"> <i class="fa-solid fa-hashtag fa-sm"></i> CNN</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/computer-science"> <i class="fa-solid fa-tag fa-sm"></i> computer-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks" rel="external nofollow noopener" target="_blank">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning" rel="external nofollow noopener" target="_blank">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 3.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <h2 id="semantic-segmentation-overview">Semantic Segmentation: Overview</h2> <p>Semantic image segmentation is the task of labelling each pixel of an image into a predefined set of classes. The output assigns a semantic class label to each pixel, so a segmented map can be drawn such that regions of the same class labelled with the same colour.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_sementation_map-480.webp 480w,/assets/img/deep/C4W3_sementation_map-800.webp 800w,/assets/img/deep/C4W3_sementation_map-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_sementation_map.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Semantic Segmentation as A Probability Map" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Semantic Segmentation as A Probability Map </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_FSO-1-480.webp 480w,/assets/img/deep/C4W3_FSO-1-800.webp 800w,/assets/img/deep/C4W3_FSO-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_FSO-1.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Semantic Segmentation as A Colour Labelled Map" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Semantic Segmentation as A Colour Labelled Map </div> <p>Applications of semantic segmentations include</p> <ul> <li>Autonomous vehicles: help car distinguish roads, lanes, pedestrians and obstacles for safer navigation</li> <li>Medical imaging: distinguish organs, tumours and tissues with high precision for diagnostics</li> <li>Satellite imagery: map land use, model cities, analyse urban development, monitor water bodies, etc.</li> <li>Augmented reality and photography: enable live background replacement, portrait modes and advanced filters</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_segmentaion-in-real-time-480.webp 480w,/assets/img/deep/C4W3_segmentaion-in-real-time-800.webp 800w,/assets/img/deep/C4W3_segmentaion-in-real-time-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_segmentaion-in-real-time.gif" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Real-time Semantic Segmentation for Autonomous Vehicles" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Real-time Semantic Segmentation for Autonomous Vehicles </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_medical-image-segmentation_feature_Image-480.webp 480w,/assets/img/deep/C4W3_medical-image-segmentation_feature_Image-800.webp 800w,/assets/img/deep/C4W3_medical-image-segmentation_feature_Image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_medical-image-segmentation_feature_Image.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Medical Image Segmentation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Medical Image Segmentation </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_satellite-imagery-480.webp 480w,/assets/img/deep/C4W3_satellite-imagery-800.webp 800w,/assets/img/deep/C4W3_satellite-imagery-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_satellite-imagery.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Semantic Segmentation for Satellite Imagery" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Semantic Segmentation for Satellite Imagery </div> <h2 id="fully-convolutional-networks-fcns">Fully Convolutional Networks (FCNs)</h2> <p>Like other computer vision tasks, we use a CNN for semantic segmentation. However, unlike image classification problems, where the size of the input image gets <strong>downsampled</strong> through a series of strided convolutional and pooling layers and is finally fed into fully-connected layers for classification purposes, semantic segmentation problems requires the output have the same resolution as the input image.</p> <p>To retain the spatial information that is lost during the downsampling phase, we replace the fully-connected layers in the network by a series of <strong>upsampling</strong> layers followed by more convolutional layers to reproduce higher resolution feature maps. This architecture is called a <strong>Fully Convolutional Network (FCN)</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_segmentation-CNN-480.webp 480w,/assets/img/deep/C4W3_segmentation-CNN-800.webp 800w,/assets/img/deep/C4W3_segmentation-CNN-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_segmentation-CNN.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Fully Convolutional Network (FCN)" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fully Convolutional Network (FCN) </div> <p>The reason behind this general architecture is the following. A typical CNN starts with a high resolution image so it is impractical to connect each neuron to all other neurons. Hence, the initial layers in a CNN can only capture information about smaller regions of the image and learn low-level features like lines, edges and colours. As the feature map is passed through more layers, the size of the image keeps on decreasing and the number of the channels keeps on increasing. Despite the loss of spatial information, the deeper layers become able to learn high-level features like faces and objects. These high-level information about the input image is contained in its various channels.</p> <p>Now that we have obtained this low-resolution tensor, we have to increase its resolution up to the original image to achieve the task of semantic segmentation. During the upsampling phase, the resolution of the image gets restored while the number of the channels in the feature maps decreases.</p> <h4 id="transpose-convolution">Transpose Convolution</h4> <p><strong>Transpose convolution</strong>, also called fractionally-strided convolution, is a type of CNN layer that is useful for tasks that involve upsampling. Instead of sliding the filter over the input, a transposed convolutional layer slides the input over the filter. The element-wise multiplication and summations are performed in a similar way.</p> <p>The example below illustrates how the transposed convolution with a \(2\times2\) filter is computed for a \(2\times2\) input tensor with a stride of \(1\) and no padding.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_transpose-convolution-480.webp 480w,/assets/img/deep/C4W3_transpose-convolution-800.webp 800w,/assets/img/deep/C4W3_transpose-convolution-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_transpose-convolution.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Transpose Convolution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Transpose Convolution </div> <p>The next example illustrates how the transposed convolution is computed with stride \(s=2\) and padding \(p=1\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_transpose-convolution-with-stride-480.webp 480w,/assets/img/deep/C4W3_transpose-convolution-with-stride-800.webp 800w,/assets/img/deep/C4W3_transpose-convolution-with-stride-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_transpose-convolution-with-stride.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Transpose Convolution with Strides" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Transpose Convolution with Strides </div> <p>In general, if an input feature map of size \(n_h\times n_w\) is passed through a weighted filter of size \(f \times f\) with a stride of \(s\) and padding of \(p\), then the output of the transposed convolutional layer will be:</p> \[\left( (n_h-1)\times s + f - 2p \right) \times \left( (n_w-1)\times s + f - 2p \right)\] <p>This could result in an output that is larger than the input, and hence increase the spatial dimensions of the feature maps.</p> <h2 id="u-net">U-Net</h2> <p><strong>U-Net</strong>, named for its U-shape, was originally created for tumour detection, but has now become a popular choice for many other semantic segmentation tasks.</p> <p>U-Net improves on the FCN, using a somewhat similar design, but differing in some important ways. It uses a matching number of convolutions for downsampling and transposed convolutions for upsampling. It also adds <strong>skip connections</strong>, to retain information that would otherwise become lost during encoding. Skip connections send information to every upsampling layer in the decoder from the corresponding downsampling layer in the encoder, capturing finer information while also keeping computation low. These help prevent information loss, as well as model overfitting.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_u-net-architecture-480.webp 480w,/assets/img/deep/C4W3_u-net-architecture-800.webp 800w,/assets/img/deep/C4W3_u-net-architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_u-net-architecture.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="U-Net" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> U-Net Architecture </div> <h4 id="u-net-model-details">U-Net: Model Details</h4> <p>In the programming assignment of the course, we got to build our own U-Net for image segmentation. The architecture of this particular U-Net is shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_unet-480.webp 480w,/assets/img/deep/C4W3_unet-800.webp 800w,/assets/img/deep/C4W3_unet-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_unet.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="U-Net" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> U-Net Model </div> <ul> <li> <p><strong>Contracting path</strong> (Encoder containing downsampling steps): The contracting path follows a regular CNN architecture to downsample the image and extract its features. In detail, it consists of the repeated application of two \(3\times3\) same padding convolutions, each followed by a ReLU unit and a \(2\times2\) max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled.</p> </li> <li> <p><strong>Crop function</strong>: This step crops the image from the contracting path and concatenates it to the current image on the expanding path to create a skip connection.</p> </li> <li> <p><strong>Expanding path</strong> (Decoder containing upsampling steps): The expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually. In detail, each step in the expanding path upsamples the feature map, followed by a \(2\times2\) transposed convolution. This transposed convolution halves the number of feature channels, while growing the height and width of the image. Next is a concatenation with the correspondingly cropped feature map from the contracting path, and two \(3\times3\) convolutions, each followed by a ReLU.</p> </li> <li> <p><strong>Final Feature Mapping Block</strong>: In the final layer, a \(1\times1\) convolution is used to map each 64-component feature vector to the desired number of classes. By choosing an appropriate number of \(1\times1\) filters, the channel dimensions can be reduced to have one layer per class.</p> </li> </ul> <p>The U-Net network has 23 convolutional layers in total.</p> <h4 id="experimental-results">Experimental Results</h4> <p>The U-Net model for semantic image segmentation is implemented with <strong>sparse categorical cross entropy</strong> for pixelwise multiclass prediction and trained on on the <strong>CARLA</strong> self-driving car dataset.</p> <p>Although the model was only trained for 40 epochs due to computational constraints for the assignment, we get some pretty amazing results.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_unet-result-2-480.webp 480w,/assets/img/deep/C4W3_unet-result-2-800.webp 800w,/assets/img/deep/C4W3_unet-result-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_unet-result-2.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Training Results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Training Results 1 </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_unet-result-3-480.webp 480w,/assets/img/deep/C4W3_unet-result-3-800.webp 800w,/assets/img/deep/C4W3_unet-result-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W3_unet-result-3.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Training Results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Training Results 2 </div> </div> </article> </div> <style>.img-natural{max-width:90%;width:auto!important;height:auto;display:block;margin-left:auto;margin-right:auto}h2{color:var(--global-bg-color);background:var(--global-theme-color);line-height:1.6em;border-radius:3px;padding:2px;display:inline-block}</style> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Colin Young. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>