<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Training Deeper CNNs (Deep Learning Notes C4W2) | Colin Young's Personal Site </title> <meta name="author" content="Colin Young"> <meta name="description" content="residual networks (ResNets), depthwise separable convolutions and further advices"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W2-deeper-CNN/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Colin Young's Personal Site </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">home </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">logs </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Training Deeper CNNs (Deep Learning Notes C4W2)</h1> <p class="post-meta"> Created on November 27, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/cnn"> <i class="fa-solid fa-hashtag fa-sm"></i> CNN</a>   <a href="/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> computer-vision</a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/computer-science"> <i class="fa-solid fa-tag fa-sm"></i> computer-science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks" rel="external nofollow noopener" target="_blank">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning" rel="external nofollow noopener" target="_blank">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 2.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <hr> <h2 id="residual-networks-resnets">Residual Networks (ResNets)</h2> <p>Deeper networks tend to have better performance (can represent more complex functions, and also can learn features at many different levels of abstraction), but the network would suffer from problems of vanishing or exploding gradients as it goes deeper.</p> <p>ResNets introduce <strong>shortcuts</strong>, or <strong>skip connections</strong>, across two or more layers. Stacking ResNet blocks on top of each other makes training very deep neural networks possible.</p> <p>The diagram shows a skip connection being added between the \(l^\text{th}\) layer and the \((l+2)^\text{th}\) layer, which can be symbollically represented as: \(a^{[l+2]} = g\left( z^{[l+2]} + a^{[l]}\right)\)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_skip_connection-480.webp 480w,/assets/img/deep/C4W2_skip_connection-800.webp 800w,/assets/img/deep/C4W2_skip_connection-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_skip_connection.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Skip Connection" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Skip Connection </div> <h4 id="why-resnets-work">Why ResNets work?</h4> <p>A residual block can easily learn an <strong>identity function</strong>. By tuning the weights \(w^{[l+2]}\) and \(b^{[l+2]}\) to zero, then the activation \(z^{[l+2]} = w^{[l+2]} a^{[l+1]} + b^{[l+2]} = 0\), so \(a^{[l+2]} = \text{ReLU}\left(a^{[l]}\right) = a^{[l]}\). This means that adding these two layers have little risk of harming the overall performance of the neural network.</p> <h4 id="typical-residual-blocks">Typical residual blocks</h4> <ul> <li> <strong>Identity block</strong>: input activation has the same dimention as output activation</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_idblock3_kiank-480.webp 480w,/assets/img/deep/C4W2_idblock3_kiank-800.webp 800w,/assets/img/deep/C4W2_idblock3_kiank-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_idblock3_kiank.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="The Identity Block" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Identity Block </div> <ul> <li> <strong>Convolutional block</strong>: dimenstions of input and output do not match up, so an additional convolutional layer is needed in the shortcut path to adjust the dimension of the previous activations before forwarding them to the upcoming layer</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_convblock_kiank-480.webp 480w,/assets/img/deep/C4W2_convblock_kiank-800.webp 800w,/assets/img/deep/C4W2_convblock_kiank-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_convblock_kiank.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="The Convolutional Block" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Convolutional Block </div> <h2 id="depthwise-separable-convolutions">Depthwise Separable Convolutions</h2> <p>Traditional convolutions can be very resource intensive, and <strong>depthwise separable convolution</strong>s can reduce the number of trainable parameters and operations, and hence speed up the computations.</p> <p>For normal convolution operation, suppose the input data has dimensions \(n^{[l]} \times n^{[l]}\times n_C^{[l]}\) and the output data has dimensions \(n^{[l+1]} \times n^{[l+1]}\times n_C^{[l+1]}\), and we are using \(N = n_C^{[l+1]}\) filters of size \(f \times f \times n_C^{[l]}\), then the total number of multiplication in this convolution operation is: \(N \times {n^{[l+1]}}^2 \times f^2 \times n_C^{[l]}\).</p> <p>Let’s see how the computations can be greatly reduced by breaking down this single convolution operation into a depthwise convolution followed by a pointwise convolution.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_depthwise_separable-480.webp 480w,/assets/img/deep/C4W2_depthwise_separable-800.webp 800w,/assets/img/deep/C4W2_depthwise_separable-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_depthwise_separable.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Normal Convolution v.s. Depthwise Separable Convolution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Normal Convolution v.s. Depthwise Separable Convolution </div> <p>In depthwise step, instead of applying convolution to all the \(n_C^{[l]}\) channels, the convolution is applied to a single channel at a time. So the filters will be of the size \(f \times f \times 1\) and \(n_C^{[l]}\) of such filters are required. The number of multiplications at depthwise step is: \(n_C^{[l]} \times {n^{[l+1]}}^2 \times f^2\).</p> <p>Next, in the pointwise step, a \(1\times1\) convolution is applied on the \(n_C^{[l]}\) channels. So the filter size for this operation is \(1 \times 1 \times n_C^{[l]}\), and we would need \(N=n_C^{[l+1]}\) such filters to match the dimension of output data. The number of multiplications at point-wise step is: \(N \times n_C^{[l]} \times {n^{[l+1]}}^2\).</p> <p>For the overall depthwise separable operation, the total number of multiplications: \(n_C^{[l]} \times {n^{[l+1]}}^2 \times (f^2+N)\). png Comparing normal convolution operation with depthwise separable convolution, we find:</p> \[\frac{\text{\#. of mul. of depthwise separable convolution}}{\text{\#. of mul. of normal convolution}} = \frac{f^2 + N}{N \times f^2} = \frac{1}{N} + \frac{1}{f^2}\] <p>Take \(N=512\) and \(f=5\) as an example, the ratio is found to be ~4.2%, so this depthwise seperable block performs over 20 times fewer multiplications as compared to a normal convolutional block. This suggests that we can deploy faster convolution neural network models without losing much of the accuracy.</p> <h4 id="mobilenetv2-architecture">MobileNetV2 Architecture</h4> <p>The diagram below shows the architecture of MobileNetV2, which takes advantage of depthwise separable convolutions together with shortcut connections to speed up training and improve predictions. This allows MobileNetV2 to be run on mobile or other low-power applications with good efficiency.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_mobilenetv2-480.webp 480w,/assets/img/deep/C4W2_mobilenetv2-800.webp 800w,/assets/img/deep/C4W2_mobilenetv2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_mobilenetv2.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="MobileNetV2 Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> MobileNetV2 Architecture </div> <h2 id="inception-networks">Inception Networks</h2> <p>We can apply different convolutions and pooling with filters of multiple sizes at the same layer, and concatenate them to give an output volume.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_inception_block-480.webp 480w,/assets/img/deep/C4W2_inception_block-800.webp 800w,/assets/img/deep/C4W2_inception_block-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_inception_block.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Inception Blocks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Inception Blocks </div> <p>One problem with such inception block is its high computation cost. In order to save computations, we can shrink the number of channels by using \(1\times1\) convolution filters. The following example shows how the number of computations is greatly reduced by the bottleneck layer of \(1\times1\) convolutions.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_reduce_cost-480.webp 480w,/assets/img/deep/C4W2_reduce_cost-800.webp 800w,/assets/img/deep/C4W2_reduce_cost-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_reduce_cost.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="1×1 Convolution Filters" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 1×1 Convolution Filters </div> <h2 id="further-advices">Further Advices</h2> <h4 id="transfer-learning">Transfer Learning</h4> <p>Many pre-trained models have been trained on very large datasets and have learned those weights with optimized hyperparameters.</p> <p>For our own particular problem, instead of training a NN from scratch, we can use a specific NN architecture that has been trained by someone else. By replacing the output layer with a new one while keeping all the previous layers fixed, we only need to fine tune the last layer to fit the training examples. We can even compute the last activation for all training examples and save them to disk to reduce computations.</p> <p>If we have enough data and computation power, instead of starting training a NN with random initialisations, we can initialise the weights with the parameters from pre-trained models and run optimisation algorithms from there. This, in general, can greatly reduce the amount of model training time.</p> <h4 id="data-augmentation">Data Augmentation</h4> <p>Data augmentation is the process of artificially generating new data from existing data. This method helps us to have more training examples when we do not have enough data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_data_augmentation-480.webp 480w,/assets/img/deep/C4W2_data_augmentation-800.webp 800w,/assets/img/deep/C4W2_data_augmentation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/deep/C4W2_data_augmentation.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Examples of Data Augmentation Methods" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Examples of Data Augmentation Methods </div> <p>Common data augmentation methods used for computer vision tasks include:</p> <ul> <li>geometric transformations (mirroring, random cropping, rotation, shearing, etc.)</li> <li>colour space transformations (add RGB distortions to the image, change contrast, change brightness, etc.)</li> </ul> <p>It is also possible to add noise or randomly erase some part of the image to improve the model performance.</p> </div> </article> </div> <style>.img-natural{max-width:90%;width:auto!important;height:auto;display:block;margin-left:auto;margin-right:auto}h2{color:var(--global-bg-color);background:var(--global-theme-color);line-height:1.6em;border-radius:3px;padding:2px;display:inline-block}</style> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Colin Young. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>