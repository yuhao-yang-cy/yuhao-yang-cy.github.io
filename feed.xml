<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yuhao-yang-cy.github.io//feed.xml" rel="self" type="application/atom+xml"/><link href="https://yuhao-yang-cy.github.io//" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-23T03:09:54+00:00</updated><id>https://yuhao-yang-cy.github.io//feed.xml</id><title type="html">Colin Young’s Personal Site</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Neural Style Transfer (Deep Learning Notes C4W4)</title><link href="https://yuhao-yang-cy.github.io//blog/2026/deep-C4W4-NST/" rel="alternate" type="text/html" title="Neural Style Transfer (Deep Learning Notes C4W4)"/><published>2026-01-06T21:01:00+00:00</published><updated>2026-01-06T21:01:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2026/deep-C4W4-NST</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2026/deep-C4W4-NST/"><![CDATA[<blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 4.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <h2 id="neural-style-transfer-overview">Neural Style Transfer: Overview</h2> <p>The goal of <strong>Neural Style Transfer</strong> (NST) is to merge two images, a <strong>content image</strong> (\(C\)) and a <strong>style image</strong> (\(S\)), to create a <strong>generated image</strong> (\(G\)) such that the generated image \(G\) combines the content of the image \(C\) with the style of image \(S\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_style_transfer-480.webp 480w,/assets/img/deep/C4W4_style_transfer-800.webp 800w,/assets/img/deep/C4W4_style_transfer-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_style_transfer.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Nerual Style Transfer" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Nerual Style Transfer </div> <p>This is implemented by extracting the content statistics of the content image \(C\) and the style statistics of the style image \(S\) using a deep convolutional network, and then optimizing the output image \(G\) to match these statistics.</p> <h2 id="choosing-the-layers-of-the-model">Choosing the Layers of the Model</h2> <p>Starting from the network’s input layer, the first few layer activations represent low-level features like edges, corner and simple textures. As we step through the network, the deeper layers detect higher-level features like parts of specific objects and more complex patterns.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_viz_cnns-480.webp 480w,/assets/img/deep/C4W4_viz_cnns-800.webp 800w,/assets/img/deep/C4W4_viz_cnns-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_viz_cnns.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Layers of a CNN" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Layers of a Typical CNN </div> <p>Nerual Style Transfer uses <em>intermediate layers</em> of a pre-trained image classification network such as the VGG19 network, so that the content representation contains information about the image’s structures and objects while discarding specific details that are not relevant to the main objective. Choosing intermediate layers allows the NST algorithm to control the balance between preserving the content image’s structure and applying the style image’s aesthetic.</p> <h2 id="cost-functions-for-nst">Cost Functions for NST</h2> <p>The cost function that we use for the NST algorithm contains two terms:</p> <ul> <li>a <strong>content cost function</strong> \(J_\text{content}(C,G)\) that measures the similiarity between the contents of \(C\) and \(G\)</li> <li>a <strong>style cost function</strong> \(J_\text{style}(S,G)\) that measures the similarity between the styles of \(S\) and \(G\)</li> </ul> <p>Then the total cost is defined as the linear combination of the two</p> \[J(G) = \alpha J_\text{content}(C,G) + \beta J_\text{style}(S,G)\] <h2 id="content-cost-function-j_textcontentcg">Content Cost Function \(J_\text{content}(C,G)\)</h2> <p>The generated image G should match the content of image C. The content cost function can be defined as:</p> \[J_\text{content}(C,G) = \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2 \tag{1}\] <p>Here, \(n_H, n_W\) and \(n_C\) are the height, width and number of channels of the intermediate hidden layer we have chosen, and appear as a normalization term in the cost. The terms \(a^{(C)}\) and \(a^{(G)}\) are the 3D volumes corresponding to a hidden layer’s activations of the neural network. The content cost then measures how different \(a^{(C)}\) and \(a^{(G)}\) are. When we minimize the content cost later, this will help make sure \(G\) has similar contents as \(C\).</p> <p>In order to compute the cost \(J_\text{content}(C,G)\), it might also be convenient to <em>unroll</em> these 3D volumes into a 2D matrix, as shown below. Technically this unrolling step is not necessary for computing \(J_\text{content}\), but it will be good practice for when we carry out a similar operation later for computing the style cost \(J_\text{style}\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_NST_Unrolling-480.webp 480w,/assets/img/deep/C4W4_NST_Unrolling-800.webp 800w,/assets/img/deep/C4W4_NST_Unrolling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_NST_Unrolling.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Unrolling a 3D Layer" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Unrolling a 3D Layer </div> <h2 id="style-matrix">Style Matrix</h2> <p>The style of an image can be represented using the Gram matrix of an intermediate layer’s activations.</p> <p>In linear algebra, the <strong>Gram matrix</strong> \(\mathbf{G}\) of a set of vectors \((v_{1},\dots ,v_{n})\) is the matrix of dot products, whose entries are</p> \[\mathbf{G}_{ij} = v_{i}^T v_{j} = \text{np.dot}(v_{i}, v_{j})\] <p>In NST, we compute the Style matrix by multiplying the unrolled filter matrix with its transpose</p> \[\mathbf{G}_\text{gram} = \mathbf{A}_\text{unrolled} \mathbf{A}_\text{unrolled}^T\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_NST_GM-480.webp 480w,/assets/img/deep/C4W4_NST_GM-800.webp 800w,/assets/img/deep/C4W4_NST_GM-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_NST_GM.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Gram Matrix Multiplication" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Gram Matrix Multiplication </div> <p>In components, supopse the activation at row \(i\), column \(j\) and channel \(k\) at a chosen hidden layer is represented by \(a_{i,j,k}\), then the matrix element of \(\mathbf{G}_\text{gram}\) is:</p> \[\mathbf{G}_{\text{(gram)}{k,k'}} = \sum_{i=1}^{n_H} \sum_{j=1}^{n_W} a_{i,j,k} \,a_{i,j,k'}\] <p>The result will be a matrix of dimension \(n_C \times n_C\) where \(n_C\) is the number of channels of that intermediate layer.</p> <p>The matrix element \(\mathbf{G}_{k,k'}\) compares how similar the activations from the filter \(k\) is to those activations from the filter \(k'\). If \(a_k\) and \(a_{k'}\) are highly correlated, their dot product should be large and thus we expect the matrix element \(\mathbf{G}_{k,k'}\) to be large.</p> <p>Also, the diagonal elements \(\mathbf{G}_{k,k}\) measures the level of activity of the filter \(k\). If \(\mathbf{G}_{k,k}\) is large, then this means the image contains a lot of features as represented by the channel \(k\).</p> <p>By capturing the prevalence of different types of features (\(\mathbf{G}_{k,k}\)) as well as how much different features occur together (\(\mathbf{G}_{k,k'}\)), the matrix \(\mathbf{G}_\text{gram}\) measures the style of the image.</p> <h2 id="style-cost">Style Cost</h2> <p>After calculating the Gram matrix, the next goal is to minimize the distance between the Gram matrix of the style image \(S\) and the Gram matrix of the generated image \(G\).</p> <p>For a single hidden layer, we can compute the Gram matrices of the style image \(S\) and the generated image \(G\), that is \(\mathbf{G}_\text{gram}^{(S)}\) and \(\mathbf{G}_\text{gram}^{(G)}\), using the activations \(a^{[l]}\) from this particular intermediate layer in the network. The corresponding style cost is defined as:</p> \[J_\text{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{k=1}^{n_C}\sum_{k'=1}^{n_C}(\mathbf{G}^{(S)}_{\text{(gram)}k,k'} - \mathbf{G}^{(G)}_{\text{(gram)}k,k'})^2\tag{2}\] <p>Better results can be obtained if the representation of the style image \(S\) is computed from multiple different layers and combined. The style costs from several different layers can be merged to give:</p> \[J_\text{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_\text{style}(S,G)\] <p>where \(\lambda^{[l]}\) are the weights that reflect how much each layer contributes to the style and they are subject to the normalisation condition \(\sum_{l=1}^L\lambda^{[l]} = 1\).</p> <p>The choice of the coefficients for each layer depends on how much we want the generated image to follow the style image. Since deeper layers capture higher-level concepts, and the features in the deeper layers are less localized in the image relative to each other, so if we want to follow the style image softly, larger weights for deeper layers and smaller weights for the earlier layers can be chosen. In contrast, if we want the generated image to strongly follow the style image, we may choose smaller weights for deeper layers and larger weights for the earlier layers.</p> <h2 id="total-cost">Total Cost</h2> <p>The content cost and the style cost functions are combined to give the total cost:</p> \[J(G) = \alpha J_\text{content}(C,G) + \beta J_\text{style}(S,G)\] <p>Here \(\alpha\) and \(\beta\) are hyperparameters that control the relative weighting between content and style.</p> <p>By loading the VGG19 model and choosing a suitable optimizer for the total cost, we are able to implement Neural Style Transfer and generate artistic images with lots of fun!</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_antelope_NST-480.webp 480w,/assets/img/deep/C4W4_antelope_NST-800.webp 800w,/assets/img/deep/C4W4_antelope_NST-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_antelope_NST.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Style Transfer with Customised Images" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Style Transfer with Customised Images </div>]]></content><author><name></name></author><category term="machine-learning"/><category term="computer-science"/><category term="machine-learning"/><category term="deep-learning"/><category term="CNN"/><category term="computer-art"/><summary type="html"><![CDATA[generation of an image that blends the content from one image and the style of the other]]></summary></entry><entry><title type="html">Face Recognition (Deep Learning Notes C4W4)</title><link href="https://yuhao-yang-cy.github.io//blog/2026/deep-C4W4-face-recognition/" rel="alternate" type="text/html" title="Face Recognition (Deep Learning Notes C4W4)"/><published>2026-01-05T19:42:00+00:00</published><updated>2026-01-05T19:42:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2026/deep-C4W4-face-recognition</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2026/deep-C4W4-face-recognition/"><![CDATA[<blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 4.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <h2 id="face-recognition-overview">Face Recognition: Overview</h2> <p>Face recognition technology uses AI to analyze unique facial features, creating a mathematical encoding to identify or verify a person, common in access control systems like contactless entrance guards and phone unlocking (Face ID).</p> <p>Face recognition problems commonly fall into one of the two categories:</p> <ul> <li><strong>Face Verification</strong> <ul> <li>Input: image, name/ID</li> <li>Output: whether the input image corresponds to the claimed person (1-to-1 matching problem)</li> </ul> </li> <li><strong>Face Recognition</strong> <ul> <li>Input: image</li> <li>Output: whether the input image corresponds to any one of the registered persons in a database (1-to-\(K\) matching problem)</li> </ul> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_face_recog_tasks-480.webp 480w,/assets/img/deep/C4W4_face_recog_tasks-800.webp 800w,/assets/img/deep/C4W4_face_recog_tasks-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_face_recog_tasks.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Face Recognition Tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Face Recognition Tasks </div> <h2 id="one-shot-learning">One-shot Learning</h2> <p>One of the difficulties in this approach is that we might only have one example image for each person to train the model. This is called <strong>one-shot learning</strong>.</p> <p>Also, the model has to be retrained each time a new person is added to the database, and in test time, we need to compare an input image to all the instances in the database.</p> <p>One possible solution for the one-shot learning problem is to learn a <strong>similarity function</strong>:</p> \[d(\text{img1}, \text{img2}) = \text{degree of difference between the input images}\] <ul> <li>If \(d(\text{img1}, \text{img2})\) is less than some threshold value, then we claim the two images show the same person.</li> <li>If \(d(\text{img1}, \text{img2})\) is greater than the threshold, then the two images show two different persons.</li> </ul> <h2 id="siamese-network">Siamese Network</h2> <p>To learn a similarity function, we use a <strong>Siamese Network</strong>, which takes a person’s image \(x\) as the input and outputs an <strong>encoding</strong> \(f(x)\). In the course’s programming assignment, we used a 128-dimensional encoding in the output layer, i.e., the output is a vector of 128 components.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_f_x-480.webp 480w,/assets/img/deep/C4W4_f_x-800.webp 800w,/assets/img/deep/C4W4_f_x-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_f_x.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Encoding of a Face Image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Encoding of a Face Image </div> <p>The goal of training is to learn parameters so that</p> <ul> <li>If \(x^{(i)}\) and \(x^{(j)}\) are the same person, then \(\| f(x^{(i)}) - f(x^{(j)}) \|^2\) is small.</li> <li>If \(x^{(i)}\) and \(x^{(j)}\) are different persons, then \(\| f(x^{(i)}) - f(x^{(j)}) \|^2\) is large.</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_siamese-480.webp 480w,/assets/img/deep/C4W4_siamese-800.webp 800w,/assets/img/deep/C4W4_siamese-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_siamese.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Siamese Network for Face Recognition Tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Siamese Network for Face Recognition Tasks </div> <h2 id="triplet-loss">Triplet Loss</h2> <p>The <strong>triplet loss</strong> is an effective loss function for training a neural network to learn an encoding of a face image. In particular, training of the network use triplets of images \((A, P, N)\), that is:</p> <ul> <li><strong>Anchor</strong> \(A\): an image of a person</li> <li><strong>Positive</strong> \(P\): an image of the same person as the Anchor</li> <li><strong>Negative</strong> \(N\): an image of a different person</li> </ul> <p>The network will try to learn parameters that pushes the encodings of \(A\) and \(P\) closer together while pulling the encodings of \(A\) and \(N\) further apart. For even better differentiation, we would like to make sure that the Anchor image \(A\) is closer to the Positive image \(P\) than to the Negative image \(N\) by at least some margin \(\alpha\).</p> \[d(A, P) + \alpha &lt; d(A, N)\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_triplet-loss-480.webp 480w,/assets/img/deep/C4W4_triplet-loss-800.webp 800w,/assets/img/deep/C4W4_triplet-loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_triplet-loss.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Triplet Loss" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Triplet Loss </div> <p>If we measure the similarity with an \(L_2\) distance, then this becomes</p> \[\| f(A)-f(P)\|^2+\alpha &lt; \| f(A)-f(N)\|^2\] <p>With a training dataset of many triplets, we would like to minimise the following triplet cost:</p> \[\mathcal{J} = \sum^{m}_{i=1} \text{max} \left( \left[ \| f(A^{(i)}) - f(P^{(i)}) \|^2 - \| f(A^{(i)}) - f(N^{(i)}) \|^2 + \alpha \right], 0 \right)\] <p>where \((A^{(i)}, P^{(i)}, N^{(i)})\) denotes the \(i\)-th training example.</p> <h2 id="binary-classification">Binary Classification</h2> <p>An alternative to the triplet loss approach is to formulate the face recognition problem as a binary classification problem. The CNN computes the encodings of two input images \(x^{(i)}\) and \(x^{(j)}\), and use a logistic regression unit to produce an output:</p> \[\hat{y} = \sigma\left( \sum_{k=1}^m \left( w_i | f(x^{(i)})_k - f(x^{(j)})_k| + b \right) \right)\] <ul> <li>\(\hat{y} = 1\) if the two images are of the same person</li> <li>\(\hat{y} = 0\) if the two images are of the same person</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W4_distance_kiank-480.webp 480w,/assets/img/deep/C4W4_distance_kiank-800.webp 800w,/assets/img/deep/C4W4_distance_kiank-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W4_distance_kiank.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Binary Classification" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Binary Classification </div>]]></content><author><name></name></author><category term="machine-learning"/><category term="computer-science"/><category term="machine-learning"/><category term="deep-learning"/><category term="CNN"/><category term="computer-vision"/><summary type="html"><![CDATA[training a Siamese network for face recognition tasks with a triplet loss function]]></summary></entry><entry><title type="html">Object Detection (Deep Learning Notes C4W3)</title><link href="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-object-detection/" rel="alternate" type="text/html" title="Object Detection (Deep Learning Notes C4W3)"/><published>2025-12-30T13:50:00+00:00</published><updated>2025-12-30T13:50:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-object-detection</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-object-detection/"><![CDATA[<blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 3.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <h2 id="typical-tasks-in-computer-vision">Typical Tasks in Computer Vision</h2> <ul> <li> <p><strong>Image Classification</strong>: classify an image to a specific class (usually only one object in the whole image)</p> </li> <li> <p><strong>Classification with Localisation</strong>: learn the class of the object, and generate a bounding box to give the location of the object in the image (usually only one object in the whole image)</p> </li> <li> <p><strong>Object Detection</strong>: detect all the objects in the image and predict their classes and give their locations (usually more than one object from different classes in the image)</p> </li> <li> <p><strong>Semantic Segmentation</strong>: label each pixel in the image with a category label</p> </li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_image_segmentation-480.webp 480w,/assets/img/deep/C4W3_image_segmentation-800.webp 800w,/assets/img/deep/C4W3_image_segmentation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_image_segmentation.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Common Computer Vision Tasks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Common Computer Vision Tasks </div> <h2 id="object-detection">Object Detection</h2> <h4 id="defining-the-outputs">Defining the Outputs</h4> <p>The output vector \(y\) in the classification with localisation problem can be defined as: \(y = \left[ \begin{array}{c} P_c \\ b_x \\ b_y \\ b_w \\ b_h \\ C_1 \\ C_2 \\ \vdots \\ C_s \end{array} \right]\) where</p> <ul> <li>\(P_c\) gives the probability if there is an object in the image</li> <li>\(b_x\) and \(b_y\) give the coordinates of the centre of the object</li> <li>\(b_w\) and \(b_h\) are the width and height of the object</li> <li>\(C_1, C_2, \cdots , C_s\) are the probabilities that the object belongs to each of the \(s\)​ classes</li> </ul> <p>The output \(y\) can also be defined to be: \(y = \left[ \begin{array}{c} P_c \\ b_x \\ b_y \\ b_w \\ b_h \\ c \end{array} \right]\) where \(c\) is the class index that represents one of the \(s\) classes to which the object belongs.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_box_label-480.webp 480w,/assets/img/deep/C4W3_box_label-800.webp 800w,/assets/img/deep/C4W3_box_label-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_box_label.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Output Vector" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Output Vector for the Problem </div> <h4 id="landmarks">Landmarks</h4> <p>In computer vision problems like face recognition problems, we might also want to output some points on the face like corners of the eyes, corners of he mouth, corners of the nose and so on, which makes it possible to predict the facial expressions of that person. Another example is when we get the skeleton of a person, outputting the positions of the hands, elbows, shoulders, knees, feet and so on could be helpful in predicting what the person is doing. This is called <strong>landmark detection</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_Facial%20Landmark%20Detection-480.webp 480w,/assets/img/deep/C4W3_Facial%20Landmark%20Detection-800.webp 800w,/assets/img/deep/C4W3_Facial%20Landmark%20Detection-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_Facial%20Landmark%20Detection.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Facial Landmarks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Facial Landmarks </div> <p>Instead of using four numbers to give the location and the size of the object of interest, a collection of landmarks are needed in the labelled data. \(y=\left[ \begin{array}{c} P_c \\ l_{1,x} \\ l_{1,y} \\ l_{2,x} \\ l_{2,y} \\ \vdots \\ l_{s,x} \\ l_{s,y} \end{array} \right]\) where \((l_{i,x}, l_{i,y})\) are the coordinates of the \(i^\text{th}\) landmark which can be the left corner of the left eye, or the right corner of the nose, etc.</p> <h4 id="choosing-the-loss-function">Choosing the Loss Function</h4> <p>There are two primary tasks in an object detection problem: <strong>classification</strong> (identifying the class of the object) and <strong>localisation</strong> (specifying the object’s position and size). In practice, we use a combination of loss functions.</p> <p>In the course, Andrew’s recommendations are:</p> <ul> <li>logistic regression for \(P_c\) and log-likelihood loss for the classes (standard loss function for most classification tasks)</li> <li>mean squared error for the bounding boxes</li> </ul> <h4 id="sliding-window-technique">Sliding Window Technique</h4> <p>For object detection problems, there could be many objects scattered at multiple places across an image, but we can build upon what we had learned earlier in this course and use the sliding window technique.</p> <p>Recall that we have learned how to train a CNN for the image classification problems, that is to identity the only object in one image. For object detection, we can crop specific windows of the images (with varying sizes and varying ratios) and forward the cropped portion into a CNN and predict the corresponding class for each window.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_sliding-window-in-action-480.webp 480w,/assets/img/deep/C4W3_sliding-window-in-action-800.webp 800w,/assets/img/deep/C4W3_sliding-window-in-action-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_sliding-window-in-action.gif" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Sliding Windows" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Sliding Window Technique </div> <p>The disadvantage of this method is its high computational cost. It turns out that the sliding windows can be implemented more efficiently using convolutional networks.</p> <h4 id="sliding-window-with-convolutional-implementation">Sliding Window with Convolutional Implementation</h4> <p>The illustration below shows how a convolutional network takes a \(16\times16\times3\) image and produces a \(2\times2\times4\) output. Each \(1\times1\times4\) slice in the output makes a prediction about the probability that each corresponding sliding window (as shown in different colours) belongs to each one of the \(4\) classes. Similar architectures can be applied for bigger images and more classes.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_sliding-window-by-convolution-480.webp 480w,/assets/img/deep/C4W3_sliding-window-by-convolution-800.webp 800w,/assets/img/deep/C4W3_sliding-window-by-convolution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_sliding-window-by-convolution.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Sliding Window with Convolutional Implementation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sliding Window with Convolutional Implementation </div> <p>The sliding window approach treats each window as an independent image patch separately, but neighbouring sliding windows with large fractions of overlapping may share a lot of repeated computations. Therefore, processing the entire image as a whole with a CNN, which shares and reuses the feature map for multiple regions, can significantly eliminate redundant computations and hence reduce computational cost.</p> <h2 id="yolo">YOLO</h2> <p><strong>YOLO</strong> (<em>You Only Look Once</em>) is a popular algorithm for object detection problems because of its high accuracy and also its ability to run in real time. This algorithm was named YOLO as it requires only one forward propagation pass through the network to make predictions, so in this sense it only looks once at the image. After non-max suppression, it then outputs recognized objects together with the bounding boxes.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_yolo_on_predict_time-480.webp 480w,/assets/img/deep/C4W3_yolo_on_predict_time-800.webp 800w,/assets/img/deep/C4W3_yolo_on_predict_time-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_yolo_on_predict_time.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="YOLO in Real Time" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> YOLO in Real Time </div> <h4 id="bounding-box-prediction">Bounding Box Prediction</h4> <p>By dividing the input image into a smaller grid of size \(G \times G\), we can perform a simple object localization for each grid cell where the network outputs the class probabilities and the bounding boxes of the main object in that grid cell. For each grid cell, the network detects the object for which its centre belongs to that grid cell, even if the object may span into neighbouring grid cells.</p> <p>For each grid cell, if we define the target label as mentioned earlier as</p> \[y = \left[ \begin{array}{c} P_c \\ b_x \\ b_y \\ b_w \\ b_h \\ C_1 \\ C_2 \\ \vdots \\ C_s \end{array} \right]\] <p>then the dimension of the output layer is \(G \times G \times (s + 5)\).</p> <h4 id="anchor-boxes">Anchor Boxes</h4> <p>Objects from different classes usually have different shapes. For example, we may expect the bounding box for a pedestrian to be tall and thin, while the bounding box for a car to be relatively wider. To represent different objects in the training data, we choose reasonable height/width ratios for different classes. Such bounding boxes with predefined reference shapes are called <strong>anchor boxes</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_anchor-boxes-480.webp 480w,/assets/img/deep/C4W3_anchor-boxes-800.webp 800w,/assets/img/deep/C4W3_anchor-boxes-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_anchor-boxes.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Anchor Boxes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Anchor Boxes </div> <p>Suppose we have \(n_A\) anchor boxes, then the dimension of the output tensor of the last layer is \(G \times G \times n_A \times (s+5)\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_architecture-480.webp 480w,/assets/img/deep/C4W3_architecture-800.webp 800w,/assets/img/deep/C4W3_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_architecture.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Encoding for Multiple Anchor Boxes" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Encoding for Multiple Anchor Boxes </div> <h4 id="class-scores">Class Scores</h4> <p>We can further extract a probability that the box contains a certain class by computing the element-wise product \(P_c \times C_i\). After calculating the score for all \(s\) classes in one anchor box, we may identify the maximum score and assign to this anchor box this class score and the corresponding class.</p> <h4 id="non-max-suppression">Non-Max Suppression</h4> <p>One problem with YOLO at this stage is that it detects the same object multiple times. The model would output \(G \times G \times n_A\) boxes, which are way too many boxes. We need to reduce the algorithm’s output to a much smaller number of detected objects. To do so, we can use non-max suppression to make sure that YOLO detect each object just once. This involves the following steps:</p> <ul> <li>choose a threshold for the class scores and get rid of the boxes with a low class score</li> <li>select only one box when several boxes overlap with each other but detect the same object</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_non-max-suppression-480.webp 480w,/assets/img/deep/C4W3_non-max-suppression-800.webp 800w,/assets/img/deep/C4W3_non-max-suppression-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_non-max-suppression.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Non-Max Suppression" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Non-Max Suppression </div> <p>To determine which boxes detect the same object, one measure that can be used is the <strong>intersection over union</strong>. The <em>IoU</em> between two bounding boxes \(B_1\) and \(B_2\) is defined as:</p> \[IoU = \frac{B_1 \cap B_2}{B_1 \cup B_2}\] <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_IoU-480.webp 480w,/assets/img/deep/C4W3_IoU-800.webp 800w,/assets/img/deep/C4W3_IoU-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_IoU.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="IoU" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> IoU (Intersection over Union) </div> <p>While there are too many remaining boxes, we pick the box with the largest \(P_c\) and output that as a prediction. At the mean time, we search for and remove any remaining box with the same output but with an <em>IoU</em> that is greater than a certain threshold.</p> <p>If there are \(s\) classes to be detected, we should run non-max suppression \(s\) times, once for each output class.</p> <h4 id="visualising-yolo">Visualising YOLO</h4> <p>One way to visualize YOLO’s output is to plot the bounding boxes that it predicts. Doing this results in a picture like this:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_Detected-with-YOLO-480.webp 480w,/assets/img/deep/C4W3_Detected-with-YOLO-800.webp 800w,/assets/img/deep/C4W3_Detected-with-YOLO-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_Detected-with-YOLO.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Objects Detected with YOLO" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Objects Detected with YOLO </div> <p>Another way to visualise what YOLO predicts on an image is to do the following:</p> <ul> <li>for each grid cell, find the highest probability score (take a maximum score across the \(s\) classes, and one maximum for each of the \(n_A\) anchor boxes)</li> <li>colour that grid cell according to what object that grid cell considers the most likely</li> </ul> <p>Doing this results in a picture that looks like this:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_proba_map-480.webp 480w,/assets/img/deep/C4W3_proba_map-800.webp 800w,/assets/img/deep/C4W3_proba_map-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_proba_map.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Colouring Labels with YOLO" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Colouring Labels with YOLO </div>]]></content><author><name></name></author><category term="machine-learning"/><category term="computer-science"/><category term="machine-learning"/><category term="deep-learning"/><category term="CNN"/><category term="computer-vision"/><summary type="html"><![CDATA[object detection and the YOLO algorithm]]></summary></entry><entry><title type="html">Image Semantic Segmentation (Deep Learning Notes C4W3)</title><link href="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-segmentation/" rel="alternate" type="text/html" title="Image Semantic Segmentation (Deep Learning Notes C4W3)"/><published>2025-12-30T13:50:00+00:00</published><updated>2025-12-30T13:50:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-segmentation</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W3-segmentation/"><![CDATA[<blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 3.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <h2 id="semantic-segmentation-overview">Semantic Segmentation: Overview</h2> <p>Semantic image segmentation is the task of labelling each pixel of an image into a predefined set of classes. The output assigns a semantic class label to each pixel, so a segmented map can be drawn such that regions of the same class labelled with the same colour.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_sementation_map-480.webp 480w,/assets/img/deep/C4W3_sementation_map-800.webp 800w,/assets/img/deep/C4W3_sementation_map-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_sementation_map.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Semantic Segmentation as A Probability Map" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Semantic Segmentation as A Probability Map </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_FSO-1-480.webp 480w,/assets/img/deep/C4W3_FSO-1-800.webp 800w,/assets/img/deep/C4W3_FSO-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_FSO-1.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Semantic Segmentation as A Colour Labelled Map" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Semantic Segmentation as A Colour Labelled Map </div> <p>Applications of semantic segmentations include</p> <ul> <li>Autonomous vehicles: help car distinguish roads, lanes, pedestrians and obstacles for safer navigation</li> <li>Medical imaging: distinguish organs, tumours and tissues with high precision for diagnostics</li> <li>Satellite imagery: map land use, model cities, analyse urban development, monitor water bodies, etc.</li> <li>Augmented reality and photography: enable live background replacement, portrait modes and advanced filters</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_segmentaion-in-real-time-480.webp 480w,/assets/img/deep/C4W3_segmentaion-in-real-time-800.webp 800w,/assets/img/deep/C4W3_segmentaion-in-real-time-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_segmentaion-in-real-time.gif" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Real-time Semantic Segmentation for Autonomous Vehicles" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Real-time Semantic Segmentation for Autonomous Vehicles </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_medical-image-segmentation_feature_Image-480.webp 480w,/assets/img/deep/C4W3_medical-image-segmentation_feature_Image-800.webp 800w,/assets/img/deep/C4W3_medical-image-segmentation_feature_Image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_medical-image-segmentation_feature_Image.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Medical Image Segmentation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Medical Image Segmentation </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_satellite-imagery-480.webp 480w,/assets/img/deep/C4W3_satellite-imagery-800.webp 800w,/assets/img/deep/C4W3_satellite-imagery-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_satellite-imagery.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Semantic Segmentation for Satellite Imagery" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Semantic Segmentation for Satellite Imagery </div> <h2 id="fully-convolutional-networks-fcns">Fully Convolutional Networks (FCNs)</h2> <p>Like other computer vision tasks, we use a CNN for semantic segmentation. However, unlike image classification problems, where the size of the input image gets <strong>downsampled</strong> through a series of strided convolutional and pooling layers and is finally fed into fully-connected layers for classification purposes, semantic segmentation problems requires the output have the same resolution as the input image.</p> <p>To retain the spatial information that is lost during the downsampling phase, we replace the fully-connected layers in the network by a series of <strong>upsampling</strong> layers followed by more convolutional layers to reproduce higher resolution feature maps. This architecture is called a <strong>Fully Convolutional Network (FCN)</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_segmentation-CNN-480.webp 480w,/assets/img/deep/C4W3_segmentation-CNN-800.webp 800w,/assets/img/deep/C4W3_segmentation-CNN-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_segmentation-CNN.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Fully Convolutional Network (FCN)" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fully Convolutional Network (FCN) </div> <p>The reason behind this general architecture is the following. A typical CNN starts with a high resolution image so it is impractical to connect each neuron to all other neurons. Hence, the initial layers in a CNN can only capture information about smaller regions of the image and learn low-level features like lines, edges and colours. As the feature map is passed through more layers, the size of the image keeps on decreasing and the number of the channels keeps on increasing. Despite the loss of spatial information, the deeper layers become able to learn high-level features like faces and objects. These high-level information about the input image is contained in its various channels.</p> <p>Now that we have obtained this low-resolution tensor, we have to increase its resolution up to the original image to achieve the task of semantic segmentation. During the upsampling phase, the resolution of the image gets restored while the number of the channels in the feature maps decreases.</p> <h4 id="transpose-convolution">Transpose Convolution</h4> <p><strong>Transpose convolution</strong>, also called fractionally-strided convolution, is a type of CNN layer that is useful for tasks that involve upsampling. Instead of sliding the filter over the input, a transposed convolutional layer slides the input over the filter. The element-wise multiplication and summations are performed in a similar way.</p> <p>The example below illustrates how the transposed convolution with a \(2\times2\) filter is computed for a \(2\times2\) input tensor with a stride of \(1\) and no padding.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_transpose-convolution-480.webp 480w,/assets/img/deep/C4W3_transpose-convolution-800.webp 800w,/assets/img/deep/C4W3_transpose-convolution-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_transpose-convolution.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Transpose Convolution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Transpose Convolution </div> <p>The next example illustrates how the transposed convolution is computed with stride \(s=2\) and padding \(p=1\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_transpose-convolution-with-stride-480.webp 480w,/assets/img/deep/C4W3_transpose-convolution-with-stride-800.webp 800w,/assets/img/deep/C4W3_transpose-convolution-with-stride-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_transpose-convolution-with-stride.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Transpose Convolution with Strides" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Transpose Convolution with Strides </div> <p>In general, if an input feature map of size \(n_h\times n_w\) is passed through a weighted filter of size \(f \times f\) with a stride of \(s\) and padding of \(p\), then the output of the transposed convolutional layer will be:</p> \[\left( (n_h-1)\times s + f - 2p \right) \times \left( (n_w-1)\times s + f - 2p \right)\] <p>This could result in an output that is larger than the input, and hence increase the spatial dimensions of the feature maps.</p> <h2 id="u-net">U-Net</h2> <p><strong>U-Net</strong>, named for its U-shape, was originally created for tumour detection, but has now become a popular choice for many other semantic segmentation tasks.</p> <p>U-Net improves on the FCN, using a somewhat similar design, but differing in some important ways. It uses a matching number of convolutions for downsampling and transposed convolutions for upsampling. It also adds <strong>skip connections</strong>, to retain information that would otherwise become lost during encoding. Skip connections send information to every upsampling layer in the decoder from the corresponding downsampling layer in the encoder, capturing finer information while also keeping computation low. These help prevent information loss, as well as model overfitting.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_u-net-architecture-480.webp 480w,/assets/img/deep/C4W3_u-net-architecture-800.webp 800w,/assets/img/deep/C4W3_u-net-architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_u-net-architecture.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="U-Net" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> U-Net Architecture </div> <h4 id="u-net-model-details">U-Net: Model Details</h4> <p>In the programming assignment of the course, we got to build our own U-Net for image segmentation. The architecture of this particular U-Net is shown below.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_unet-480.webp 480w,/assets/img/deep/C4W3_unet-800.webp 800w,/assets/img/deep/C4W3_unet-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_unet.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="U-Net" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> U-Net Model </div> <ul> <li> <p><strong>Contracting path</strong> (Encoder containing downsampling steps): The contracting path follows a regular CNN architecture to downsample the image and extract its features. In detail, it consists of the repeated application of two \(3\times3\) same padding convolutions, each followed by a ReLU unit and a \(2\times2\) max pooling operation with stride 2 for downsampling. At each downsampling step, the number of feature channels is doubled.</p> </li> <li> <p><strong>Crop function</strong>: This step crops the image from the contracting path and concatenates it to the current image on the expanding path to create a skip connection.</p> </li> <li> <p><strong>Expanding path</strong> (Decoder containing upsampling steps): The expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually. In detail, each step in the expanding path upsamples the feature map, followed by a \(2\times2\) transposed convolution. This transposed convolution halves the number of feature channels, while growing the height and width of the image. Next is a concatenation with the correspondingly cropped feature map from the contracting path, and two \(3\times3\) convolutions, each followed by a ReLU.</p> </li> <li> <p><strong>Final Feature Mapping Block</strong>: In the final layer, a \(1\times1\) convolution is used to map each 64-component feature vector to the desired number of classes. By choosing an appropriate number of \(1\times1\) filters, the channel dimensions can be reduced to have one layer per class.</p> </li> </ul> <p>The U-Net network has 23 convolutional layers in total.</p> <h4 id="experimental-results">Experimental Results</h4> <p>The U-Net model for semantic image segmentation is implemented with <strong>sparse categorical cross entropy</strong> for pixelwise multiclass prediction and trained on on the <strong>CARLA</strong> self-driving car dataset.</p> <p>Although the model was only trained for 40 epochs due to computational constraints for the assignment, we get some pretty amazing results.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_unet-result-2-480.webp 480w,/assets/img/deep/C4W3_unet-result-2-800.webp 800w,/assets/img/deep/C4W3_unet-result-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_unet-result-2.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Training Results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Training Results 1 </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W3_unet-result-3-480.webp 480w,/assets/img/deep/C4W3_unet-result-3-800.webp 800w,/assets/img/deep/C4W3_unet-result-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W3_unet-result-3.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Training Results" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Training Results 2 </div>]]></content><author><name></name></author><category term="machine-learning"/><category term="computer-science"/><category term="machine-learning"/><category term="deep-learning"/><category term="CNN"/><category term="computer-vision"/><summary type="html"><![CDATA[labelling and colouring the pixels of an image into a set of predefined classes]]></summary></entry><entry><title type="html">Training Deeper CNNs (Deep Learning Notes C4W2)</title><link href="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W2-deeper-CNN/" rel="alternate" type="text/html" title="Training Deeper CNNs (Deep Learning Notes C4W2)"/><published>2025-11-27T16:47:00+00:00</published><updated>2025-11-27T16:47:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/deep-C4W2-deeper-CNN</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/deep-C4W2-deeper-CNN/"><![CDATA[<blockquote> <p>These are some notes for the Coursera Course on <a href="https://www.coursera.org/learn/convolutional-neural-networks">Convolutional Neural Networks</a>, which is a part of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization</a>. This post is a summary of the course contents that I learned from Week 2.</p> <p>These notes are far from original. All credits go to the course instructor Andrew Ng and his team.</p> </blockquote> <hr/> <h2 id="residual-networks-resnets">Residual Networks (ResNets)</h2> <p>Deeper networks tend to have better performance (can represent more complex functions, and also can learn features at many different levels of abstraction), but the network would suffer from problems of vanishing or exploding gradients as it goes deeper.</p> <p>ResNets introduce <strong>shortcuts</strong>, or <strong>skip connections</strong>, across two or more layers. Stacking ResNet blocks on top of each other makes training very deep neural networks possible.</p> <p>The diagram shows a skip connection being added between the \(l^\text{th}\) layer and the \((l+2)^\text{th}\) layer, which can be symbollically represented as: \(a^{[l+2]} = g\left( z^{[l+2]} + a^{[l]}\right)\)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_skip_connection-480.webp 480w,/assets/img/deep/C4W2_skip_connection-800.webp 800w,/assets/img/deep/C4W2_skip_connection-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_skip_connection.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Skip Connection" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Skip Connection </div> <h4 id="why-resnets-work">Why ResNets work?</h4> <p>A residual block can easily learn an <strong>identity function</strong>. By tuning the weights \(w^{[l+2]}\) and \(b^{[l+2]}\) to zero, then the activation \(z^{[l+2]} = w^{[l+2]} a^{[l+1]} + b^{[l+2]} = 0\), so \(a^{[l+2]} = \text{ReLU}\left(a^{[l]}\right) = a^{[l]}\). This means that adding these two layers have little risk of harming the overall performance of the neural network.</p> <h4 id="typical-residual-blocks">Typical residual blocks</h4> <ul> <li><strong>Identity block</strong>: input activation has the same dimention as output activation</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_idblock3_kiank-480.webp 480w,/assets/img/deep/C4W2_idblock3_kiank-800.webp 800w,/assets/img/deep/C4W2_idblock3_kiank-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_idblock3_kiank.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="The Identity Block" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Identity Block </div> <ul> <li><strong>Convolutional block</strong>: dimenstions of input and output do not match up, so an additional convolutional layer is needed in the shortcut path to adjust the dimension of the previous activations before forwarding them to the upcoming layer</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_convblock_kiank-480.webp 480w,/assets/img/deep/C4W2_convblock_kiank-800.webp 800w,/assets/img/deep/C4W2_convblock_kiank-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_convblock_kiank.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="The Convolutional Block" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The Convolutional Block </div> <h2 id="depthwise-separable-convolutions">Depthwise Separable Convolutions</h2> <p>Traditional convolutions can be very resource intensive, and <strong>depthwise separable convolution</strong>s can reduce the number of trainable parameters and operations, and hence speed up the computations.</p> <p>For normal convolution operation, suppose the input data has dimensions \(n^{[l]} \times n^{[l]}\times n_C^{[l]}\) and the output data has dimensions \(n^{[l+1]} \times n^{[l+1]}\times n_C^{[l+1]}\), and we are using \(N = n_C^{[l+1]}\) filters of size \(f \times f \times n_C^{[l]}\), then the total number of multiplication in this convolution operation is: \(N \times {n^{[l+1]}}^2 \times f^2 \times n_C^{[l]}\).</p> <p>Let’s see how the computations can be greatly reduced by breaking down this single convolution operation into a depthwise convolution followed by a pointwise convolution.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_depthwise_separable-480.webp 480w,/assets/img/deep/C4W2_depthwise_separable-800.webp 800w,/assets/img/deep/C4W2_depthwise_separable-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_depthwise_separable.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Normal Convolution v.s. Depthwise Separable Convolution" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Normal Convolution v.s. Depthwise Separable Convolution </div> <p>In depthwise step, instead of applying convolution to all the \(n_C^{[l]}\) channels, the convolution is applied to a single channel at a time. So the filters will be of the size \(f \times f \times 1\) and \(n_C^{[l]}\) of such filters are required. The number of multiplications at depthwise step is: \(n_C^{[l]} \times {n^{[l+1]}}^2 \times f^2\).</p> <p>Next, in the pointwise step, a \(1\times1\) convolution is applied on the \(n_C^{[l]}\) channels. So the filter size for this operation is \(1 \times 1 \times n_C^{[l]}\), and we would need \(N=n_C^{[l+1]}\) such filters to match the dimension of output data. The number of multiplications at point-wise step is: \(N \times n_C^{[l]} \times {n^{[l+1]}}^2\).</p> <p>For the overall depthwise separable operation, the total number of multiplications: \(n_C^{[l]} \times {n^{[l+1]}}^2 \times (f^2+N)\). png Comparing normal convolution operation with depthwise separable convolution, we find:</p> \[\frac{\text{\#. of mul. of depthwise separable convolution}}{\text{\#. of mul. of normal convolution}} = \frac{f^2 + N}{N \times f^2} = \frac{1}{N} + \frac{1}{f^2}\] <p>Take \(N=512\) and \(f=5\) as an example, the ratio is found to be ~4.2%, so this depthwise seperable block performs over 20 times fewer multiplications as compared to a normal convolutional block. This suggests that we can deploy faster convolution neural network models without losing much of the accuracy.</p> <h4 id="mobilenetv2-architecture">MobileNetV2 Architecture</h4> <p>The diagram below shows the architecture of MobileNetV2, which takes advantage of depthwise separable convolutions together with shortcut connections to speed up training and improve predictions. This allows MobileNetV2 to be run on mobile or other low-power applications with good efficiency.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_mobilenetv2-480.webp 480w,/assets/img/deep/C4W2_mobilenetv2-800.webp 800w,/assets/img/deep/C4W2_mobilenetv2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_mobilenetv2.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="MobileNetV2 Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> MobileNetV2 Architecture </div> <h2 id="inception-networks">Inception Networks</h2> <p>We can apply different convolutions and pooling with filters of multiple sizes at the same layer, and concatenate them to give an output volume.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_inception_block-480.webp 480w,/assets/img/deep/C4W2_inception_block-800.webp 800w,/assets/img/deep/C4W2_inception_block-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_inception_block.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Inception Blocks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Inception Blocks </div> <p>One problem with such inception block is its high computation cost. In order to save computations, we can shrink the number of channels by using \(1\times1\) convolution filters. The following example shows how the number of computations is greatly reduced by the bottleneck layer of \(1\times1\) convolutions.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_reduce_cost-480.webp 480w,/assets/img/deep/C4W2_reduce_cost-800.webp 800w,/assets/img/deep/C4W2_reduce_cost-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_reduce_cost.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="1×1 Convolution Filters" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 1×1 Convolution Filters </div> <h2 id="further-advices">Further Advices</h2> <h4 id="transfer-learning">Transfer Learning</h4> <p>Many pre-trained models have been trained on very large datasets and have learned those weights with optimized hyperparameters.</p> <p>For our own particular problem, instead of training a NN from scratch, we can use a specific NN architecture that has been trained by someone else. By replacing the output layer with a new one while keeping all the previous layers fixed, we only need to fine tune the last layer to fit the training examples. We can even compute the last activation for all training examples and save them to disk to reduce computations.</p> <p>If we have enough data and computation power, instead of starting training a NN with random initialisations, we can initialise the weights with the parameters from pre-trained models and run optimisation algorithms from there. This, in general, can greatly reduce the amount of model training time.</p> <h4 id="data-augmentation">Data Augmentation</h4> <p>Data augmentation is the process of artificially generating new data from existing data. This method helps us to have more training examples when we do not have enough data.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/deep/C4W2_data_augmentation-480.webp 480w,/assets/img/deep/C4W2_data_augmentation-800.webp 800w,/assets/img/deep/C4W2_data_augmentation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/deep/C4W2_data_augmentation.png" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Examples of Data Augmentation Methods" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Examples of Data Augmentation Methods </div> <p>Common data augmentation methods used for computer vision tasks include:</p> <ul> <li>geometric transformations (mirroring, random cropping, rotation, shearing, etc.)</li> <li>colour space transformations (add RGB distortions to the image, change contrast, change brightness, etc.)</li> </ul> <p>It is also possible to add noise or randomly erase some part of the image to improve the model performance.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="computer-science"/><category term="machine-learning"/><category term="deep-learning"/><category term="CNN"/><category term="computer-vision"/><summary type="html"><![CDATA[residual networks (ResNets), depthwise separable convolutions and further advices]]></summary></entry><entry><title type="html">CIE A2 Physics Paper 5 实验设计 + 范文×8</title><link href="https://yuhao-yang-cy.github.io//blog/2025/a2phy-planning/" rel="alternate" type="text/html" title="CIE A2 Physics Paper 5 实验设计 + 范文×8"/><published>2025-05-15T23:45:00+00:00</published><updated>2025-05-15T23:45:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/a2phy-planning</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/a2phy-planning/"><![CDATA[<blockquote> <p>碎碎念：这篇文章的上一版已经是<a href="https://mp.weixin.qq.com/s/HCjkcfk23ToSJZm_P-3Ivg">7年前发布</a>的了，拿出来缝缝补补，做点排版精装，又可以改头换面，当作新鲜热乎的教案垃圾登堂入室了！</p> </blockquote> <p>CIE A-Level 物理的 Paper 5 包含两个实验技能相关的大题。Question 1 关于<strong>实验方案设计（Planning）</strong>，Question 2 关于数据处理和误差分析（Analysis &amp; Evaluation）。今天我们针对 Question 1，也就是实验设计题，介绍下它的考察内容和答题策略。</p> <p>简单来说，实验设计题会给出一个待验证的定量规律，通常是明确地以一个方程式的形式给出。考生需要设计完整的实验方案，需要什么器材，如何使用相关的仪器测得数据，如何提高测量的精度和数据的可靠性，操作过程中有何需要注意的安全事项，以及最后获得数据如何处理从而验证待证明的结果，这些都需要考生详细地在报告中一一阐述。</p> <p>根据大纲，实验方案设计题总共15分，分配大致如下。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_f1-480.webp 480w,/assets/img/a_levels/9702_P5_f1-800.webp 800w,/assets/img/a_levels/9702_P5_f1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_f1.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 截取自 CIE A-Level Physics (9702) Syllabus </div> <p>我们接下来具体介绍每一项，并在最后附上 6 篇烤羚羊的独家范文，供同学们参考。</p> <h2 id="问题定义-defining-problem">问题定义 Defining Problem</h2> <p>在报告中，考生需要指出实验的<strong>自变量（independent variable）</strong>以及<strong>应变量（dependent variable）</strong>，即在操作中先改变谁，接着观察另一个物理量会对应地变化。前者即为自变量，后者为应变量。如果实验中存在其他也会影响应变量的因素，考生也应当明确指出，实验过程中需要采用<strong>控制变量法（variable to be kept constant）</strong>。</p> <p>例如，实验要求验证单摆（simple pendulum）的周期与摆线长度的关系。摆线长度即为 independent variable，单摆周期即为 dependent variable。单摆球的质量、每次释放单摆的偏移角度应该控制不变。</p> <p>又例如，如果实验要求验证线圈中心的磁场强度反比于线圈的半径。那么线圈半径就是 independent variable，产生的磁场强度为 dependent variable。通过线圈的电流、线圈的匝数应当控制不变。</p> <p>更普遍的，如果实验要求验证某个应变量 \(y\) 和相应的自变量 \(x\) 具有形如 \(y=f(x, p, q, \cdots)\) 的函数关系（其中 \(p, q, \cdots\) 是某些在问题中有明确定义的物理量），那么在 Question 1 的开头，我们就可以写下：</p> <p>​ \(x\) is the independent variable.</p> <p>​ \(y\) is the dependent variable.</p> <p>​ \(p, q, \cdots\) should be kept constants.</p> <h2 id="数据采集-data-collection">数据采集 Data Collection</h2> <p>如何测量数据可能是整个报告中最需要考生智慧的一个部分，也是决定 Question 1 最终能拿到什么档次分数的重中之重。</p> <p>然而，这部分的书写应该也是条理非常明晰的。针对在问题定义中已经明确指出的自变量、应变量和不变量，我们依次针对这里的每个量的观测过程进行分别展开，它们分别用什么仪器、什么方案，如何准确测出？考生需要图文结合，将这些问题交待清楚。</p> <p>在整个 Question 1 中，数据采集部分的篇幅时占比最大，分值也是最高的。算上每套题固定的 4 个的得分点，再加上 additional details 中可以最多拉满到 6 个得分点，这个部分的分值一般会占到整个实验设计题的一半以上。</p> <h4 id="实验装置示意图">实验装置示意图</h4> <p>首当其冲的，就是装置的<strong>示意图（labelled diagram）</strong>。仪器的名称、放在哪里使用，全都可以在示意图上标出。这张图画得好，文字部分甚至只需要很少的笔墨，就可以把实验步骤阐释清楚，从而拿到高分。</p> <p>也需要提醒各位同学，大家在画示意图时，通常不会忘记画出实验所需的核心装置，但是却很有可能会疏忽同样需要的<strong>辅助装置</strong>。</p> <p>例如设计一个斜面相关的运动学实验，不要整一块全凭一口气斜架在半空的木板，记得画出如何固定它的具体方案，用铁架台（stand）和夹子（clamp）来夹住架起的那端，或者画出在木板一端的下方作垫高，任何可行方案都是可以接受的。</p> <p>再例如热学实验中如果需要对烧杯（beaker）内的液体进行加热，不要只单单只在液体里画一个用爱发电的加热器（heater），记得给它画上完整的电路图（circuit diagram），其中应该包含给它供电的电源（power supply）和其它测量需要的仪表。</p> <p>有的考生总是纠结自己画功太渣，画出来的东西根本看不出是什么鬼画符。这完全不必担心，判卷人更看重的是示意图中的<strong>标注（label）</strong>。所谓 labelled diagram，不管你画的是阿猫还是阿狗，只要你边上打了一个 label 写上这是一棵树，那它就是一棵树。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_f2-480.webp 480w,/assets/img/a_levels/9702_P5_f2-800.webp 800w,/assets/img/a_levels/9702_P5_f2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_f2.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 一幅非常精美的羚羊手绘图 </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_f3-480.webp 480w,/assets/img/a_levels/9702_P5_f3-800.webp 800w,/assets/img/a_levels/9702_P5_f3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_f3.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 一幅同样精美的羚羊手绘图 </div> <p>在判卷人眼里，以上两幅图没有任何差异。</p> <h4 id="测量方法叙述">测量方法叙述</h4> <p>接着配合示意图，就可以把每个量分别怎么测量步骤的细节罗列出来了。</p> <p>在动笔之前，建议同学们不妨思考下如下的这些问题来构建思路：</p> <ul> <li>有什么具体的方法来改变自变量？</li> <li>调节好自变量后该用什么仪器对其进行测量？</li> <li>相应变化的应变量该用什么仪器测量？</li> <li>测量自变量和应变量时，有什么方法可以减少测量的不确定度/提升测量的精度？</li> <li>有什么方法可以使得实验现象更加明显、更容易观测？</li> <li>需要控制的不变量，我们如何监控它们确实全程都保持恒定？</li> <li>实验中是否有可能导致实验人受到伤害、或是导致实验仪器损坏的安全隐患？有什么可行的防护措施？</li> </ul> <p>例如刚才提及的单摆周期的实验，我们需要米尺（metre rule）去测量摆线长度，需要秒表（stopwatch）去测量周期。并且考虑到人的反应时间误差（human reaction error），我们应当测量5次或10次振动的总时间，然后取平均值作为结果（repeat and average）。此外，我们也可以利用参照物、画标线（marker），确保每次释放单摆时的振幅都是相同的。</p> <p>例如，某个实验需要测量声音的频率（frequency）。要测声音，我们需要一个麦克风（microphone）将声音信号转换成电信号，然后连到一个示波器（oscilloscope）上观察。通过在示波器上读取波形的横向幅度，结合示波器在水平方向刻度所代表的时间（time-base setting）可以计算获得声波的周期 \(T\)，对其取倒数就可以间接算出频率 \(f=\frac{1}{T}\)。这些具体细节在报告中都应体现出来。</p> <p>再比如，某个实验需要控制温度为常数。那么我们就需要水浴或油浴（water/oil bath）来控温，并且在每次测量时充分搅拌（stir）确保待测物体与水/油浴达到热平衡（thermal equilibrium），同时通过温度计（thermometer）来监控温度。</p> <p>在实验中可能出现的风险应该采取的安全防范措施（safety precaution），也属于可以得分的点。例如，涉及大电流的实验，防止电路过热（overheating），可以指出在每次测量记录完毕后应当即时关闭电路。涉及尖锐物体、玻璃的实验，可以佩戴防护手套（gloves）。涉及物体从高空坠落的实验，可以使用软垫（cushion）、沙坑等作缓冲。涉及高温物体的热学实验，可以指出应该用钳子（tong）来操作。</p> <p>当然，不同的实验需要测量的物理量各不相同，力、电、磁、光、声、热等各个物理分支相关实验的具体方案还得具体设计。限于篇幅，其他的实例不作赘述，可以参考文末贴出的范文示例。</p> <h2 id="数据分析-data-analysis">数据分析 Data Analysis</h2> <p>实验数据采集完毕后，最后就是怎么处理数据，从而来验证试题给出的关系式了。</p> <p>处理多组数据，一种有效的方法就是画<strong>拟合曲线（best-fit line）</strong>。如果通过一些简单的数学变化，应变量和自变量之间额关系式可以写成某种线性函数（linear function）的形式，然后就可以叙述该画一幅怎样的数据图，通常的句式就是 plot \(Y\) against \(X\)，其中 \(X\) 和 \(Y\) 是自变量 \(x\) 和应变量 \(y\) 经过合理的数学变换之后的形式，分别画到数据图的横轴和纵轴。</p> <p>这里也不需要考生真的在卷面上画这张图，只需要大家用文字说清楚应该画谁关于谁的图就好。毕竟全程纸上谈兵，我们也没有做过任何的测量，这里我也不是很不明白有些同学还能整出一副图究竟是哪里变出来的数据。</p> <p>对应于数据图，我们可以脑补，如果所有的数据点落在一条直线（straight line）上，那么按照剧情设定就可以说，原关系式是成立的（relationship is valid）。如果需要验证的关系等价于某个正比例关系，也记得强调拟合线应该是过原点的直线（straight line through origin）。</p> <p>关系式中涉及到的一些待定参数、比例系数，也可以通过拟合直线的<strong>斜率（gradient）</strong>、<strong>截距（\(y\)-intercept）</strong>间接算出。注意问题的措辞一般是问该如何确定这些待定参数，所以答题只是写出斜率和/或截距的表达式是拿不到分的，一定要给出某某参数的表达式才行，当然这个表达式通常会是带有斜率和/或截距的半文字半符号的形式。</p> <p>一些常见的应变量和自变量的关系及其对应的数据处理方案可见下表。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_f4-480.webp 480w,/assets/img/a_levels/9702_P5_f4-800.webp 800w,/assets/img/a_levels/9702_P5_f4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_f4.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 如何处理和分析实验数据以验证不同形式的关系式的一些示例 </div> <h2 id="额外细节-additional-details">额外细节 Additional Details</h2> <p>其实，在写完数据分析的部分后，整篇 Question 1 的答题就应该已经结束了。考纲和评分标准中单独列出的所谓的额外细节，其实是对定义问题、数据采集、数据分析处理这三大部分的补充。它可以是某个不怎么起眼的需要控制的变量，可以是测量仪器使用中应注意的细节，可以是控制周围环境因素不要影响结果的小操作，可以是为了让实验结果更明显的小 trick，可以是实验中应当注意的安全防范措施，也可以是数据处理过程中会遇到的某个数学变换。只要言之有理，写出一点，就可以得一分，封顶6分。</p> <p>我个人的建议是，额外细节直接融合在上述问题定义、实验操作和数据处理的部分中一起写掉。在社交媒体上，有很多老师分享的范文中，会像官方评分标准那样把 Additional Details 单摘出来一条一条地写，我对这样的教法是非常反感的。对于考试拿分没啥问题，但是我觉得任何有基本科学素养的人都不会按这种结构来写实验设计方案。一气呵成的实验设计方案，就应该是从定义问题，到叙述每个相关的物理量该分别如何测量，最后阐明收集到的数据进行怎样的处理可以验证某个关系。</p> <p>最后提醒各位同学，在写报告时，不必完全按照上述的顺序来写。P5 的每个得分点都是独立的，只要你在文章的任何位置提到了关键信息，就可以拿到相应的分数。例如，你就是喜欢上来先讲数据的分析处理，顺便提一下安全事项，然后讲数据怎么测，完了再来定义问题，最后一看发现还有几条实验操作的细节可以补充，没事，CIE 判卷人虽然会读得浑身不爽，但他不能因此扣你的分。也就是说，在写作时，如果忽然想到之前的内容存在疏漏，不用慌，最后另起一段补充上即可。</p> <h2 id="范文-sample-answers">范文 Sample Answers</h2> <p>泼上 8 篇完整的范文，仅作参考。</p> <p>友情提示：手机屏幕阅读文字偏小，建议保存原图用图片浏览器打开阅读。</p> <h4 id="19702-febmar-2016-paper-52">1、9702 Feb/Mar 2016 Paper 52</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex11-480.webp 480w,/assets/img/a_levels/9702_P5_ex11-800.webp 800w,/assets/img/a_levels/9702_P5_ex11-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex11.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex12-480.webp 480w,/assets/img/a_levels/9702_P5_ex12-800.webp 800w,/assets/img/a_levels/9702_P5_ex12-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex12.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex13-480.webp 480w,/assets/img/a_levels/9702_P5_ex13-800.webp 800w,/assets/img/a_levels/9702_P5_ex13-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex13.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="29702-mayjune-2023-paper-51">2、9702 May/June 2023 Paper 51</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex21-480.webp 480w,/assets/img/a_levels/9702_P5_ex21-800.webp 800w,/assets/img/a_levels/9702_P5_ex21-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex21.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex22-480.webp 480w,/assets/img/a_levels/9702_P5_ex22-800.webp 800w,/assets/img/a_levels/9702_P5_ex22-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex22.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex23-480.webp 480w,/assets/img/a_levels/9702_P5_ex23-800.webp 800w,/assets/img/a_levels/9702_P5_ex23-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex23.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="39702-mayjune-2024-paper-51">3、9702 May/June 2024 Paper 51</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex31-480.webp 480w,/assets/img/a_levels/9702_P5_ex31-800.webp 800w,/assets/img/a_levels/9702_P5_ex31-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex31.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex32-480.webp 480w,/assets/img/a_levels/9702_P5_ex32-800.webp 800w,/assets/img/a_levels/9702_P5_ex32-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex32.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex33-480.webp 480w,/assets/img/a_levels/9702_P5_ex33-800.webp 800w,/assets/img/a_levels/9702_P5_ex33-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex33.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="49702-octnov-2018-paper-51">4、9702 Oct/Nov 2018 Paper 51</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex41-480.webp 480w,/assets/img/a_levels/9702_P5_ex41-800.webp 800w,/assets/img/a_levels/9702_P5_ex41-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex41.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex42-480.webp 480w,/assets/img/a_levels/9702_P5_ex42-800.webp 800w,/assets/img/a_levels/9702_P5_ex42-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex42.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="59702-mayjune-2024-paper-52">5、9702 May/June 2024 Paper 52</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex51-480.webp 480w,/assets/img/a_levels/9702_P5_ex51-800.webp 800w,/assets/img/a_levels/9702_P5_ex51-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex51.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex52-480.webp 480w,/assets/img/a_levels/9702_P5_ex52-800.webp 800w,/assets/img/a_levels/9702_P5_ex52-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex52.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex53-480.webp 480w,/assets/img/a_levels/9702_P5_ex53-800.webp 800w,/assets/img/a_levels/9702_P5_ex53-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex53.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="69702-octnov-2016-paper-52">6、9702 Oct/Nov 2016 Paper 52</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex61-480.webp 480w,/assets/img/a_levels/9702_P5_ex61-800.webp 800w,/assets/img/a_levels/9702_P5_ex61-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex61.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex62-480.webp 480w,/assets/img/a_levels/9702_P5_ex62-800.webp 800w,/assets/img/a_levels/9702_P5_ex62-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex62.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex63-480.webp 480w,/assets/img/a_levels/9702_P5_ex63-800.webp 800w,/assets/img/a_levels/9702_P5_ex63-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex63.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="79702-octnov-2021-paper-51">7、9702 Oct/Nov 2021 Paper 51</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex71-480.webp 480w,/assets/img/a_levels/9702_P5_ex71-800.webp 800w,/assets/img/a_levels/9702_P5_ex71-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex71.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex72-480.webp 480w,/assets/img/a_levels/9702_P5_ex72-800.webp 800w,/assets/img/a_levels/9702_P5_ex72-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex72.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex73-480.webp 480w,/assets/img/a_levels/9702_P5_ex73-800.webp 800w,/assets/img/a_levels/9702_P5_ex73-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex73.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h4 id="89702-mayjune-2016-paper-51">8、9702 May/June 2016 Paper 51</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex81-480.webp 480w,/assets/img/a_levels/9702_P5_ex81-800.webp 800w,/assets/img/a_levels/9702_P5_ex81-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex81.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex82-480.webp 480w,/assets/img/a_levels/9702_P5_ex82-800.webp 800w,/assets/img/a_levels/9702_P5_ex82-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex82.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P5_ex83-480.webp 480w,/assets/img/a_levels/9702_P5_ex83-800.webp 800w,/assets/img/a_levels/9702_P5_ex83-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P5_ex83.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="友情链接">友情链接</h2> <p>某小镇做题家在公众号上一直分享各类数学和物理的手写刷题资源。其中 CIE A-Level 物理的卷子也没有逃过我的魔爪，其中当然也包含本文讲到的很多套 Paper 5 的整卷的手写解答。</p> <p>有需要的同学可以移步<a href="/projects/cie_physics_solutions/">这个页面</a>获取 CIE A-Level 物理真题手写解答系列的下载方式。</p> <hr/> <blockquote> <p>最后祝各位即将参加 A-Level 物理考试的童鞋们好运！</p> </blockquote>]]></content><author><name></name></author><category term="physics"/><category term="a-level"/><category term="physics"/><category term="a-level"/><category term="practicals"/><summary type="html"><![CDATA[介绍下 CIE A-Level 物理 Paper 5 实验技能卷中 Question 1 关于实验方案设计（Planning）题的考察内容和答题策略]]></summary></entry><entry><title type="html">CIE AS 物理实验技能考试套路一波流</title><link href="https://yuhao-yang-cy.github.io//blog/2025/asphy-practicals/" rel="alternate" type="text/html" title="CIE AS 物理实验技能考试套路一波流"/><published>2025-04-27T14:30:00+00:00</published><updated>2025-04-27T14:30:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/asphy-practicals</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/asphy-practicals/"><![CDATA[<p>为2025年5月参加 CAIE AS Physics Paper 3 物理实验技能考试的童鞋们送上一波助攻。</p> <blockquote> <p>碎碎念：这文章已经是炒了快10年的冷饭了，每两三年添油加醋补一点，保证不过保质期，好歹还能端上桌来，接着喂给各位有需要的同学。</p> </blockquote> <h2 id="as-物理实验技能考试概览">AS 物理实验技能考试概览</h2> <h3 id="考试信息相关">考试信息相关</h3> <ul> <li>实验考试时间：2小时</li> <li>考试内容：包含2个实验，各占20分</li> <li>满分：40分</li> <li>评分标准：A线通常在29~31分之间，往下一般每3分一档</li> </ul> <h3 id="实验技能要求">实验技能要求</h3> <ul> <li>根据要求正确使用仪器</li> <li>观察实验现象</li> <li>测量、记录实验数据（不确定度？测量精度？有效数字？）</li> <li>计算、处理实验数据（作图？拟合？比较数据？）</li> <li>评估实验过程（分析误差来源？改进措施？）</li> </ul> <h2 id="实验一">实验一</h2> <p>在实验一中，会去探究搭建起来的力学或电学系统中，某个<strong>自变量</strong>（independent variable）发生改变时，相应的物理量，即<strong>应变量</strong>（dependent variable）怎样跟着改变。通常实验要求测量5至6组数据点，进而在后续的数据分析过程中拿来画图，从而理清变量之间的定量关系。</p> <h3 id="数据表格table-of-results">数据表格（Table of Results）</h3> <p>记录实验数据的表格，在实验考试40分的总分中，占到了10分之多。那么这个这么重头戏的数据表格，在填写的过程中，有哪些注意事项呢？</p> <h4 id="表头column-heading">表头（column heading）</h4> <p>测量过程中所有的<strong>原始数据</strong>（raw data）都应在表格中有所体现。例如，测量单摆的周期 \(T\)，我们会测连续摆动10次的时间，甚至重复相同的测量再去取平均值，来减小<strong>随机误差</strong>（random error）。实验测量的原始数据是10个周期，因此在表格中应该要看到有专门列出 \(10T_1\)，\(10T_2\)。</p> <p>表格每一列顶端的抬头，应该写清对应列的物理量，以及它们的<strong>单位</strong>。例如，测量长度，就该有 L/cm。记录砝码质量，就该有 m/g。</p> <h4 id="精度precision与有效数字significant-figures">精度（Precision）与有效数字（Significant Figures）</h4> <p><strong>原始数据的精度看测量仪器</strong>。带刻度的仪器应该读到相应的<strong>最小刻度</strong>（smallest scale），数字式的仪器上读到多少位有效数字，就原封不动地记录下来。</p> <p>例如，米尺（metre rule）测出的长度，应该精确到 0.1 cm；游标卡尺（vernier calliper）和螺旋测微计（micrometer）的精度，应该记录到 0.001 cm；秒表（stopwatch）的精度，记录到 0.01 s（考虑人的操作存在反应时间，也可以只记录到 0.1 s）；量角器（protractor）的精度，记录到 1°；数字式的电流表或电压表（ammeter/voltmeter），仪器上看到几位就记录几位。</p> <p>遇到原始数据读数为整数的情况，应当用零补位到需要的精度。例如，米尺测量的长度，即使正好是 50 cm，也应记为 50.0 cm。</p> <p><strong>通过原始数据计算出来的数据</strong>（calculated quantity）<strong>，有效数字的保留，应当同有效数字最少的原始数据一致</strong>，允许多一位。</p> <p>例如，我们测得了长度量 \(L=9.0 \text{ cm}\)（2位有效数字），时间量 \(t=1.30 \text{ s}\)（3位有效数字），需要计算 \(t^2L\)，最后结果应当保留至2位（或多一位，3位）有效数字：\(t^2L= 15.21 \text{ s}^2\text{cm}\)，记为 \(15\text{ s}^2\text{cm}\)（或 \(15.2\text{ s}^2\text{cm}\)）。</p> <p>角度的三角函数值，一般取3位有效数字。</p> <p>如果实在搞不清计算数据该保留多少，可以一律取3位有效数字。</p> <h4 id="数据有效范围range">数据有效范围（Range）</h4> <p><strong>自变量的测量范围，应该尽可能的越大越好，且数据点需要分布合理</strong>。因此实验测量的6个数据点中，应该将它可以取到的最小值和最大值都测到，剩余4个在这个区间内近似平均分配。</p> <p>例如，在单摆实验中，如果实验目的要探究周期 \(T\) 与摆长 \(L\) 的关系，而实验器材提供了一个约 60 cm 高度的铁架台和足够长的摆线，那么可以脑补，在仪器允许的条件下，我们最短大致可以搓一个 50 大几厘米的单摆，最短大概可以搓到 10 cm 以内，那么我们所取的 \(L\) 的范围可以大致设定在 8 cm ~ 55 cm 的样子。确定了最大值和最小值，接着再塞进4个差不多均匀分布的数据点，很容易估计出两两之间保持 9~10 cm的间距就差不多了，于是就可以大体上规划出所需要的所有6个数据点。</p> <p>而在实际操作过程中，我们也未必需要严格按照最原先的设想来，主打一个差不多就问题不大。比如我们设想的第一个数据点是 8.0 cm，但是固定好摆线后，拿尺子一量发现差了个零点几厘米。这种时候就不要犯强迫症再去微调了，量到多少就记录下来，接着开摆（字面意思）就行。所以最后体现在数据表格 \(L/\text{cm}\) 那一栏的数据，很可能是像：7.6, 16.8, 27.0, 36.5, 46.2, 55.3 这种样子，完全 OK。</p> <h3 id="past-paper-question-examples">Past Paper Question Examples</h3> <h4 id="2016-summer-paper-33">2016 Summer Paper 33</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f1-480.webp 480w,/assets/img/a_levels/9702_P3_f1-800.webp 800w,/assets/img/a_levels/9702_P3_f1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f1.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>数据表格大体应该如下：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f2-480.webp 480w,/assets/img/a_levels/9702_P3_f2-800.webp 800w,/assets/img/a_levels/9702_P3_f2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f2.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>实验中提供的砝码有 50g×1，100g×4。那么实验中，应当注意到 \(m\) 最小可以取 50g，最大可以取 450g，这两个值必须都作测量。剩下4组数据，可以大致平均分配，比如最终测的数据点依次为：50g，150g，200g，300g，350g，450g</p> <h4 id="2014-winter-paper-33">2014 Winter Paper 33</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f3-480.webp 480w,/assets/img/a_levels/9702_P3_f3-800.webp 800w,/assets/img/a_levels/9702_P3_f3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f3.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>数据表格大体应该如下：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f4-480.webp 480w,/assets/img/a_levels/9702_P3_f4-800.webp 800w,/assets/img/a_levels/9702_P3_f4-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f4.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>注意几点：</p> <p>周期 \(T\) 需要<strong>重复测量</strong>（repeated readings），在表格中应当留出专门的列填入原始数据。</p> <p>最后根据题目要求的两列计算数据，可以按需要作相应的单位转换，避免之后计算中因为单位不统一可能导致的错误。</p> <h3 id="作图graph-plotting">作图（Graph Plotting）</h3> <p>实验一测完5至6组数据后，就该画图了。</p> <h4 id="坐标的选取axis-labels--scales">坐标的选取（Axis Labels &amp; Scales）</h4> <p>记得把横轴和纵轴标注（label）清楚，物理量和对应的单位分别是什么。</p> <p>接着根据数据的范围，选择合适的刻度（scale）。</p> <p><strong>刻度的选择应当让数据点占据尽可能大的范围（至少大于图像的一半）</strong>。同时刻度的区间一般取为 2，5，10 的倍数，便于找点。注意横轴和纵轴的起始点，都可以从非零值开始（false origin）。</p> <p>数据点在对应的位置上用小叉（fine cross）画出，偏差在半个小格以内。测量过程没出太大差错的话，所有的点应当几乎落在一条直线上。</p> <p>如果有个别点偏得非常离谱，有时间的话应当回去重新测一遍，时间不允许的话可以把它圈出，标为<strong>异常点</strong>（anomalous point），后续的数据处理中不用再管它。整张图至多只允许有一个异常点。</p> <h4 id="最佳拟合直线best-fit-line">最佳拟合直线（Best-fit Line）</h4> <p><strong>拟合线</strong>应当足够地贴近所有的数据点，并且<strong>所有数据点应均匀地分布在拟合线的两侧</strong>（even distribution on either side of best fit）。</p> <p>拟合线相当于对数据点作平均，此后的数据处理就应当把数据点通通忘记，只看拟合线。</p> <h4 id="斜率和截距的计算gradient--y-intercept">斜率和截距的计算（Gradient &amp; y-intercept）</h4> <p>计算拟合线<strong>斜率</strong>（gradient of best fit line）。在线上任取两点，两点的间距应当至少超过图像的一半，原则上是越远越好。在图上标出这两点的坐标读数，并画出巨大的直角三角形，两条直角边分别标注 \(\Delta x\) 和 \(\Delta y\)，体现在完成的报告上。</p> <p>计算拟合线<strong>截距</strong>（\(y\)-intercept）。最保险的方法，在线上取任意点 \((x, y)\)，连同先前求出的斜率m，代入直线方程解析式：\(y=mx+c\)，解出截距 \(c\)。如果你的图像 \(x\) 轴是从零点开始，也可以直接读图获得截距 \(c\)。</p> <p>最后呈现的作图应该如下示意：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f5-480.webp 480w,/assets/img/a_levels/9702_P3_f5-800.webp 800w,/assets/img/a_levels/9702_P3_f5-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f5.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>对应的斜率和截距的计算过程应该如下：</p> <blockquote> \[\text{gradient} = \frac{\Delta y}{\Delta x} = \frac{20-190}{0.880-0.550} \approx -515 \text{ cm}^2\text{s}^{-1}\] \[\text{substitute } (0.550, 190): 190 = -515 \times 0.550 + c\] \[\text{so } y\text{-intercept } c\approx 473 \text{ cm}^2\] </blockquote> <h3 id="数据处理analysing-data">数据处理（Analysing Data）</h3> <p>实验一的最后，要把拟合直线的斜率 \(m\)、截距 \(c\)，与某个物理方程式中的未知参数建立起联系，并通过先前的数据计算方程中的参数。</p> <p>例如刚才的第一个真题中，会画出弹簧长度 \(y\) 关于 \(m\sin\theta\) 的图像。题中给出的关系式为 \(y=Pm\sin\theta+Q\)，要求求出未知参数 \(P\) 和 \(Q\)。对比图像，\(P\) 即为图像斜率，\(Q\) 即为图像截距。</p> <p>再例如某个实验想要探究某个电路中地的可变电阻 \(R\) 如何影响电流 \(I\) 的大小，这两个量符合关系式 \(\frac{1}{I} = \frac{R}{P} + Q\)。如果已经作出了 \(\frac{1}{I}\) 关于 \(R\) 的线性拟合图像，则 \(\frac{1}{P}\) 等于图像斜率，即 \(P = \frac{1}{\text{gradient}}\)；而 \(Q\) 直接就是图像截距。</p> <p>注意一般这类问题的计算结果需要给出相应的单位，当心不要遗漏。</p> <p>下面给出一个胡乱编造的例子作为示范：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f6-480.webp 480w,/assets/img/a_levels/9702_P3_f6-800.webp 800w,/assets/img/a_levels/9702_P3_f6-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f6.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="实验二">实验二</h2> <p>实验二通常是一个有很多不合理设计的实验。实际操作过程中，测得数据的不确定度可能会相当大，最后比较数据得出的结论往往也值得商榷。这项实验要求考生能够估计误差，鉴别误差来源，并批判性地提出改进意见。</p> <h3 id="不确定度估计evaluation-of-uncertainty">不确定度估计（Evaluation of Uncertainty）</h3> <p>对于测量不确定度很大的物理量，我们可以通过多次测量取平均（repeat readings and average）的方法来减少误差。因此，在实验报告中，我们就应该记录下每次测量的原始数据，并且展示出求平均的计算过程。</p> <p>测量的<strong>绝对不确定度</strong>（absolute uncertainty），可以简单地取为原始数据中最大值与最小值之差的一半，即 \(\Delta X = \frac{1}{2}\left( X_\text{max} - X_\text{min} \right)\)</p> <p>CIE 在绝对不确定度这块比较粗糙，如果取为平均值和最大值的差，或者是平均值和最小值的差，也都算对。尽管实际算下来，这几种算法给出的结果会略有不同，但是 CIE 的世界里我们是不用管那么多的。</p> <p><strong>百分不确定度</strong>（percentage uncertainty）可以用绝对不确定度除以平均值来估计。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f7-480.webp 480w,/assets/img/a_levels/9702_P3_f7-800.webp 800w,/assets/img/a_levels/9702_P3_f7-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f7.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="有效数字significant-figures">有效数字（Significant Figures）</h3> <p>实验二中依然会考察同学们是否在报告中为相应数据保留了合适的有效数字。这里重复下之前提到的规则：对于计算数据（calculated quantity），有效数字的保留，应当同有效数字最少的原始数据一致（允许多一位）。</p> <p>实验二的考题中经常会出现让你说说为什么你把某个量 \(X\) 保留了多少位有效数字（Justify the number of significant figures you give to \(X\)）。答题时把计算过程中涉及到的所有原始数据的有效位数通通写流水账一般地都说一遍，最后写道结果应该有多少位，就可以了。</p> <p>具体可以看下面没事找事的例子：</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f8-480.webp 480w,/assets/img/a_levels/9702_P3_f8-800.webp 800w,/assets/img/a_levels/9702_P3_f8-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f8.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="比较数据comparing-results">比较数据（Comparing Results）</h3> <p>实验二的考试中，会针对同一个问题，测量出两组类似的数据，然后要求判断这两组数据是否能验证某种比例关系。我们可以计算出比例系数，通过比较两个比例系数是否足够接近，来对应地给出结论。</p> <p>常见的套路是：先计算两个比例系数的<strong>百分偏差</strong>（percentage difference），并与某个明确说明的标准作比较，通常取5%，10%，20% 左右。如果偏差小于选定的标准，那么认为实验数据可以验证假设的比例关系。反之，则假设的比例关系不被实验支持。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/a_levels/9702_P3_f9-480.webp 480w,/assets/img/a_levels/9702_P3_f9-800.webp 800w,/assets/img/a_levels/9702_P3_f9-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/a_levels/9702_P3_f9.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>在早些年的卷子里，作为比较标准的心理价位是由考生自己随手写的。而近些年的卷子里，CIE 会直接在问题中写明待比较的参数有多大的不确定度。若是如此，那只需把自己得到的两个值的百分偏差算好，跟卷子给的不确定度比较大小，然后相应写结论即可。</p> <p>友情提醒下一些喜欢纠结数据的同学，不要因为自己测算出来的两个值相差很多就觉得这个实验完蛋了，很可能要你验证的关系本身就并不严格成立。要记住这个实验是处处充满设计缺陷的，因此只需根据你自己的结果说支持还是不支持即可。只要你的测算数据和你下的结论是匹配的，这类问题都可以拿到全部的分。</p> <h3 id="误差分析和实验改进evaluation-of-procedures">误差分析和实验改进（Evaluation of Procedures）</h3> <p>实验二的方案，故意埋下了很多不靠谱的设计，整个实验过程会有很多可能导致实验结果不靠谱的因素。实验考试40分总分的最后8分，需要指出4条<strong>实验设计的缺陷</strong>（limitation），并给出相应的<strong>改进措施</strong>（improvement）。</p> <p>尽管不同的实验，会有完全不同的问题来源，但是以下的这些问题，我们在实验中都可以试着问问自己，是否可以做的更好?</p> <ul> <li>测量的数据是否足够得出可信的结论？ <ul> <li>实验只要求测两组数据，显然不够多，应当测量更多的数据，比较或者画图再作结论。（这条是每套卷子纯纯的白给送分题，建议每位阅读此文的考生拿到卷子后，直接翻到最后一面，不用看题，直接无脑写上 Two readings are not enough the draw the conclusion. / Take more measurements and plot a graph. 保本两分。）</li> </ul> </li> <li>实验仪器是否合适？ <ul> <li>用直尺（ruler）量圆柱体直径，显然不如用游标卡尺（vernier calliper）来得靠谱。</li> <li>用秒表（stopwatch）取掐一个过程很快的运动的时间，显然不如用电子计时的光电门/运动传感器（light gate/motion sensor）来得准确。</li> <li>用量角器（protractor）测量两根粗木棍之间的夹角，不如拿个大三角尺（large set square）两处相应的长度，再通过三角函数（trigonometric functions）来计算角度。</li> </ul> </li> <li>有什么测量过程很难控制？ <ul> <li>过程很快的运动什么时候结束？皮球回弹的最大高度在哪里？肉眼不容易看清楚，于是可以引入高速摄像机（video camera），录像后看慢动作回放（slow-motion playback）来更精确地判定。</li> <li>隔空架个量角器去测量两条异面直线的夹角，手在那里抖啊抖地都拿不稳量角器（cannot hold the protractor steady），那么我们可以索性拍个照（take photo），然后在照片上测量。</li> <li>要从相同的位置静止释放一个小球，难免会手抖（hard to release the ball from rest without applying a force），那么我们可以用一个小卡片（card gate），在初始位置挡住小球，然后突然抽走让运动开始，动作可以更加干净利落。</li> </ul> </li> <li>待测的物体本身是否具有缺陷？ <ul> <li>实验有时候会让你用橡皮泥（modelling clay）糊一个球，那多半我们捏出来的是个狗啃过似的不规则几何体，实验过程中折腾几下后甚至还会发生形变（deformation），那么我们就该用质地更坚硬的材料，例如金属，来做这个实验。</li> <li>实验还有过让你用铁丝、铜丝维一个大圈，卷一个弹簧，手残党的我们做出来的东西形状必然不规则，那么更靠谱的实验，应该去订做，或者采用市场上能买到的更高规格的金属制品。</li> </ul> </li> <li>有没有其他可能影响实验结果的因素？ <ul> <li>铁架台上固定的绳子、细线是否有松动（sliding）？是否需要拿胶带（tape）去固定？</li> <li>轻质摆球的运动是否会受到风的影响？是否需要在密闭的室内环境（closed room）中进行实验，并且关闭电扇、空调设备？</li> <li>室外的阳光是否会影响光学实验中的成像清晰度？或是光敏电阻的工作环境？是否需要在暗室（dark room）中进行实验？</li> </ul> </li> </ul> <h2 id="友情链接">友情链接</h2> <p>某小镇做题家在公众号上一直分享各类数学和物理的手写刷题资源。其中 CIE A-Level 物理的卷子也没有逃过我的魔爪，其中当然也包含本文讲到的很多套 Paper 3 的分卷的手写解答。</p> <p>有需要的同学可以移步<a href="/projects/cie_physics_solutions/">这个页面</a>获取 CIE A-Level 物理真题手写解答系列的下载方式。</p> <hr/> <blockquote> <p>最后祝各位即将参加 A-Level 物理考试的童鞋们好运！</p> </blockquote>]]></content><author><name></name></author><category term="physics"/><category term="a-level"/><category term="physics"/><category term="a-level"/><category term="practicals"/><summary type="html"><![CDATA[这文章已经是炒了快10年的冷饭了，每两三年添油加醋补一点，保证不过保质期，好歹还能端上桌来，接着喂给各位有需要的同学]]></summary></entry><entry><title type="html">论如何利用概率论科学地谈恋爱</title><link href="https://yuhao-yang-cy.github.io//blog/2025/fiancee-problem/" rel="alternate" type="text/html" title="论如何利用概率论科学地谈恋爱"/><published>2025-03-28T21:26:00+00:00</published><updated>2025-03-28T21:26:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/fiancee-problem</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/fiancee-problem/"><![CDATA[<blockquote> <p><a href="https://mp.weixin.qq.com/s/9UOqrD3c93-TqTMz2de1bA">古早公众号搬运过来</a>的灌水文章，来谈一谈一个非常有意义的课题：如何利用概率论科学地把妹！</p> <p>这个问题最早是在 <a href="https://matrix67.com/blog/">matrix67 的博客</a>上读到，据说是美国数学家 Merrill Flood 于 1949 年的某次报告中提出的，更多的讨论可参考<a href="https://en.wikipedia.org/wiki/Secretary_problem">维基百科页面</a></p> </blockquote> <p>在你的人生长河中，会有好多好多的妹纸与你相识相遇，这之中总有一个是最适合你的妹纸，问题在于我们怎样来找到这个妹纸呢？我们这里有一个非常简单粗暴的策略。在你决定认真交往之前，跟妹纸约会出来只是玩玩，请客吃饭看电影可以，但是坚决不走心！为什么呢？因为交往的目的仅仅是为了对妹纸这种奇妙的生物多一些了解。然而一旦你开始认真考虑寻找人生伴侣这件事情，只要遇到比之前玩过的妹纸条件更好、更对你胃口的，就不择手段把她追到手。在这个策略中，我们绝不接受神马什么把人姑凉甩了之后懊悔不已而低三下四地回去求复合这种事情。作为丧心病狂的死理性派，我们一旦跟妹纸分手，就从此一刀两段，恩断义绝，老死不相往来。因此，我们拿来作为样本的妹纸不能太多，不然你的真爱就很有可能被你稀里糊涂玩弄了一波感情之后，从此相忘于江湖。另一方面，作为样本的妹纸也不能太少，否则你对妹纸的了解仍然跟 Jon Snow<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> 一样多，那就很难说你能有雪亮的金坷垃之眼来找到真爱。所以这里有个把妹的最优化问题，那么为了摸清底细而约会的那些妹纸究竟应该交往多少个，才可以有最大的概率找到你的命中注定的另一半呢？</p> <p>下面我们建立数学模型。假设你这辈子能勾搭上 \(N\) 个妹纸，你决定跟最早遇到的 \(k\) 个只是单纯玩玩的态度，我们姑且称之为\(k\)-策略。那么这种策略帮助你找到真爱的概率 \(P(k)\) 有多大呢？（友情提示，对数学表示头大的童鞋到此可以直接拉到底看结论）</p> <p>在你人生中遇到的所有 \(N\) 个妹纸中，最棒的那个在第 \(n\) 个出现的概率显然是简单的 \(\frac{1}{N}\)。但要最后跟她在一起，她显然不能落在最早遇到那 \(k\) 个被玩弄的样本群里，即必须有 \(n\geq k+1\) 我们才不至于错过她。另外，为了能等到她，在她之前遇到的最好的妹纸必须要在那 \(k\) 个样本里，否则如果有比所有样本妹纸都好的姑凉，又在你遇到真爱之前出现在你生命中，你就不小心跟别人跑了。在遇到真爱前，一共遇到的 \((n-1)\) 个妹纸里面最棒的那个正巧在前 \(k\) 个里被你遇到的概率是 \(\frac{k}{n-1}\)。综上讨论，我们得到 \(k\)-策略的成功概率为：</p> \[P(k) = \sum_{n=k+1}^N \left(\frac{1}{N} \times \frac{k}{n-1}\right) = \left(\sum_{n=k+1}^N \frac{1}{n-1}\right) \times \frac{k}{N}\] <p>表达式里比较麻烦的是括号里对 \(\frac{1}{n-1}\) 的求和。我们把这个求和想象成底为单位长、高逐个减小的一系列矩形的总面积，如下图蓝色区域所示。这部分面积可以由函数 \(y=\frac{1}{n-1}\) 曲线下对应的面积来近似。当 \(N\) 足够大时，这个近似的误差会足够小。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/harmonic_series-480.webp 480w,/assets/img/harmonic_series-800.webp 800w,/assets/img/harmonic_series-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/harmonic_series.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>由此这个无从下手的求和问题就转化为一个简单的积分：</p> \[\sum_{n=k+1}^N \frac{1}{n-1} \approx \int_{k+1}^{N+1} \frac{\mathrm{d} n}{n-1} = \ln(n-1)\Big|_{k+1}^{N+1} = \ln\frac{N}{k}\] <p>于是我们得到</p> \[P(k) \approx \frac{k}{N} \ln\frac{N}{k} = -\frac{k}{N} \ln\frac{k}{N}\] <p>为求出成功概率 \(P(k)\) 的最大值，我们可以使出求极值问题的杀手锏——驻点法！在最大值处，必然有 \(P(k)\) 随 \(k\) 的导数为零： \(\frac{\mathrm{d} P(k)}{\mathrm{d} k} \approx \frac{\mathrm{d}}{\mathrm{d} k}\left(-\frac{k}{N} \ln\frac{k}{N}\right) = -\frac{1}{N}\ln\frac{k}{N} - \frac{k}{N}\frac{1}{N}\frac{N}{k} = 0\)</p> <p>化简整理后可以得到</p> \[\boxed{k \approx \frac{N}{\mathrm{e}}}\] <p>注意到 \(\mathrm{e}^{-1} \approx 37\%\)，也就是说，寻找真爱的最佳策略就是，对于你这一生中遇到的前37\%的妹纸们，随便玩玩开心就好，不必认真，更好的妹纸还在后头，所谓心急吃不了嫩豆腐。对于坚持读到这里的情窦初开的高中生读者们，这大概是最理性最冷血的早恋警告，反正现在接触的妹纸多半不是最适合你的真爱，那么当下还是沉迷学习争取上名校，到时周围就会有更多更棒的猎物下手，岂不美哉？当然，这个37\%的结论的前提条件是你这辈子能接触到的妹纸数量\(N\)足够多，如果你盘算着这辈子也不会有多少妹纸来搭理你，那么还是洗洗睡吧，概率论也帮不了你，奉劝一句，且行且珍惜。。。。</p> <h4 id="拓展思考">拓展思考</h4> <p>在处理概率的估值时，我们用到了一个求和化积分的近似技巧。在很多科学问题中，这个近似处理一些不太容易严格计算的求和问题非常管用。有兴趣的读者可以试着自动动手证明一个在处理大量微观粒子的统计平均时非常有用的 Sterling 公式： \(\ln N! \approx N\ln N - N\)</p> <h4 id="footnotes">Footnotes</h4> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>G.R.R.Martin大叔的魔幻巨作冰与火之歌里坚强地依然活着的主角之一，妹纸最爱吐槽他的一句话就是：You know nothing, Jon Snow. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="mathematics"/><category term="mathematics"/><category term="probability"/><category term="modelling"/><summary type="html"><![CDATA[简单粗暴的数学模型，确定最大概率找到此生真爱的那一位的最优策略]]></summary></entry><entry><title type="html">跨国旅游？乘坐引力快车，去哪里都仅需42分钟！</title><link href="https://yuhao-yang-cy.github.io//blog/2025/gravitational-express/" rel="alternate" type="text/html" title="跨国旅游？乘坐引力快车，去哪里都仅需42分钟！"/><published>2025-03-28T21:26:00+00:00</published><updated>2025-03-28T21:26:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/gravitational-express</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/gravitational-express/"><![CDATA[<blockquote> <p>搬运子上古年代的<a href="https://mp.weixin.qq.com/s/pOGIvxq8dzhTvjpLw1yR6Q">公众号灌水文章</a>。</p> <p>主要目的是想测试下网站上据称可以用 TikZJax 把文章里的 tikz 代码渲染成直接在网页上可以展示的 SVG 图片，但是这个功能的加载速度实在感人，所以还是复古流继续截图搬运吧。</p> </blockquote> <h2 id="引力快车">引力快车</h2> <p>这个引力快车的脑洞不记得最早是在哪里看到的了，能想起来有据可查的资料是华裔理论物理学家 <a href="https://www.kitp.ucsb.edu/zee">Anthony Zee（徐一鸿）</a>的科普著作《老人的玩具》的开篇，其中提到了这样一个结论：</p> <p>设想有一条穿过地心、直通地球另一端的隧道，列车从隧道这头启动，那么无需额外的动力，在地心引力的作用下，它就可以逐渐加速落向隧道另一头。当你经过地心之后，地心引力又会反过来对你起到减速的作用。在你几乎就要停下来时，你正好可以从隧道的另一端钻出来。假设你坐的是是能够耐得住地核可怕高温的金刚列车，一路下来不考虑摩擦带来的能量损耗，那么整个旅程仅需要大约42分钟，比现有的什么磁悬浮列车、超音速飞机甚至洲际导弹都要快。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gravity_express_1-480.webp 480w,/assets/img/gravity_express_1-800.webp 800w,/assets/img/gravity_express_1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gravity_express_1.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>更有意思的是，如果你的目的地不是在地球正对面，而是在任意的什么地方，只要你的所在地和要去的目的地之间也有这样一条直线隧道，你可以同样通过借助地心引力的帮助抵达目的地。神奇的是，这段旅程所花的时间跟你要去哪里没有关系！虽然隧道的总长变短了，但由于隧道的方向跟地心引力之间有个偏角，你的加速度会比先前小一些。在计算旅程耗费的总时间时，这两个因素正好抵消！不管这条直达隧道通往哪里，旅程所需的时间都是42分钟！</p> <p>想象下从上海出发，不论是去英国伦敦，还是美国纽约，或是到澳洲墨尔本，单程都仅需40分钟多一点！留学生福音好嘛！这么阿妹子嘤的黑科技去注册个公司，稍加包装一下，指不定还真能忽悠钱多人傻的投资者，想想是不是找到了一条意淫发家致富的光辉道路？！</p> <h2 id="经过地心的引力快车">经过地心的引力快车</h2> <p>下面我们来证明这个结论。</p> <p>我们知道，两个质量分别为 \(M\) 和 \(m\) 的点粒子在间距为 \(r\) 时之间存在万有引力</p> \[F=\frac{GMm}{r^2} \tag{1}\] <p>而对于像地球这样的大家伙，在考虑它对我们的这列要深入地心的列车能有产生多大引力时，还不能把它想当然地简单当作质点来处理。</p> <p>可以证明，对于一个质量分布均匀的球壳，如果一个质点位于其内部，来自球壳各个方向上的引力会严格抵消，也就是说，这个质点不受到任何引力。如果质点位于球壳外部，那么在计算引力时，可以认为球壳的所有质量全部集中在球心上。这两个结论是平方反比力的一个自然推论，叫做球壳定理（shell theorem），由牛顿大神最早给出严格证明。</p> <p>用方程式来表达的话，一个半径为 \(R\) 的球壳 \(M\)，位于球心距离 \(r\) 处的一个质点 \(m\) 受到的引力大小为</p> \[\left\{ \begin{array}{lcl} F=0 &amp; \qquad &amp; \text{if } r&lt;R \\ F=\frac{GMm}{r^2} &amp; \qquad &amp; \text{if } r&gt;R \end{array}\right. \tag{2}\] <p>如果我们把地球想象成一个个不同大小的球壳紧密套在一起的连续结构，那么我们的列车在距离地心为 \(r\) 时，真正对引力有贡献的仅仅是半径小于 \(r\) 的那些球壳。假设地球密度均匀，总质量为 \(M\)，那么对引力有贡献的有效质量为</p> \[M_\text{eff} = \frac{V(r)}{V(R)}M = \frac{r^3}{R^3}M \tag{3}\] <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gravity_express_2-480.webp 480w,/assets/img/gravity_express_2-800.webp 800w,/assets/img/gravity_express_2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gravity_express_2.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>此时的引力大小为</p> \[F(r) = \frac{GM_\text{eff}m}{r^2} = \frac{G\left(\frac{r^3}{R^3}M\right)m}{r^2} = \frac{GMm}{R^3}r \tag{4}\] <p>在地球表面处，引力体现为物体的重力，于是有</p> \[\frac{GMm}{R^2} = mg \quad \Rightarrow \quad \frac{GM}{R^2} = g \tag{5}\] <p>因此距地心为 \(r\) 时的所受引力大小的表达式可以简化为</p> \[F(r) = \frac{mg}{R}r \tag{6}\] <p>我们于是得到运动方程</p> \[-\frac{mg}{R}r = m\frac{\mathrm{d}^2 r}{\mathrm{d} t^2} \tag{7}\] <p>其中列车的加速度表达成了位置 \(r\) 对时间 \(t\) 的二阶导数，负号的引入是由于引力总是指向地心，而 \(r\) 的定义是从地心向外的位移，两者方向相反。方程可以进一步化为一个形式简单的二阶常微分方程</p> \[\boxed{\frac{\mathrm{d}^2 r}{\mathrm{d} t^2} = -\frac{g}{R}r} \tag{8}\] <p>注意到 \(g\)，\(R\) 都是常数，说明列车的加速度与 \(r\) 时刻有正比关系，列车将以地心为平衡位置来回作简谐振动！</p> <p>初始时刻，位于地球表面的列车开始由静止状态启动，即方程的初始条件为\(t=0\)时，\(r=R\)，\(\frac{\mathrm{d} r}{\mathrm{d} t}=0\)。不难验证满足初始条件的解为</p> \[r(t) = R\cos\left(\sqrt{\frac{g}{R}} t\right) \tag{9}\] <p>一次完整振动的周期为</p> \[T=2\pi\sqrt{\frac{R}{g}} \tag{10}\] <p>从地球一头跑去另一头，只需半个完整的周期，因此我们的引力快车单程所需的时间为</p> \[\boxed{t=\pi\sqrt{\frac{R}{g}}} \tag{11}\] <p>代入地球半径 \(R=6.4\times10^6 \text{ m}\)，地表的重力加速度 \(g=9.8 \text{ m/s}^{2}\)，</p> \[t = \pi \sqrt{\frac{6.4\times10^6}{9.8}} \approx 2540 \text{ s} \approx 42.3 \text{ min} \tag{12}\] <p>这正是我们之前介绍中提到的数值！</p> <h2 id="连接地球表面任意两地的直线隧道">连接地球表面任意两地的直线隧道</h2> <p>接下来分析更普遍的情形。对于连接地球上任意两地的直线隧道，在列车距离地心\(r\)时，它受到的引力仍然仅来自于地球内部半径小于 \(r\) 的那部分，但是对列车加速产生影响的，是这个力在列车行进方向、即隧道方向上的分力。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gravity_express_3-480.webp 480w,/assets/img/gravity_express_3-800.webp 800w,/assets/img/gravity_express_3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gravity_express_3.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>在隧道方向上建立坐标轴，并选取距离地心最近处为原点。列车每时每刻的位置也可以由一个参数即它的 \(x\) 坐标完整描述。利用 (6) 式的结论，在 \(x\) 方向上对运动状态有贡献的引力分力的大小为：</p> \[F_x = F(r)\sin\theta = \frac{mg}{R}r\sin\theta = \frac{mg}{R}x \tag{13}\] <p>由此写出运动方程：</p> \[-\frac{mg}{R}x = m\frac{\mathrm{d}^2 x}{\mathrm{d} t^2} \tag{14}\] <p>即</p> \[\boxed{\frac{\mathrm{d}^2 x}{\mathrm{d} t^2} = -\frac{g}{R}x} \tag{15}\] <p>这与 (8) 式表示的运动方程形式上完全一样。因此，列车仍将在隧道作简谐运动，列车的运动虽然不再通过地心，振动的幅度有所减小，但振动过程的周期、频率跟之前完全一样！于是每次旅程所需的时间也将是</p> \[\boxed{t=\pi\sqrt{\frac{R}{g}}} \approx 42.3 \text{ min} \tag{16}\] <h3 id="思考问题">思考问题</h3> <ul> <li>如果考虑到地球内部的密度并非严格均匀分布，地核的密度实际上要略大于地壳的密度，这将如何影响我们的计算结果？</li> <li>对于直通地球正对面的列车隧道，不计摩擦力导致的能量损失，整个旅程可以达到的最大速度是多少？跟一颗贴地人造卫星的飞行速度作比较，你有什么结论？</li> <li>试证明球壳定理：对于一个质量均匀分布的空心球壳，在其内部的质点受到球壳的引力严格为零。</li> </ul> <h3 id="推荐阅读">推荐阅读</h3> <ul> <li>徐一鸿，《爱因斯坦的玩具》</li> <li>Anthony Zee, <a href="https://www.kitp.ucsb.edu/zee/books/old-mans-toy">An Old Man’s Toy</a></li> <li>Shell Theorem: <a href="https://en.wikipedia.org/wiki/Shell_theorem">https://en.wikipedia.org/wiki/Shell_theorem</a></li> </ul>]]></content><author><name></name></author><category term="physics"/><category term="a-level"/><category term="physics"/><category term="gravity"/><category term="a-level"/><summary type="html"><![CDATA[一个有趣的结论：设想有一条穿过地球内部的超级隧道，那么在地心引力的作用下，单程仅需要大约42分钟就可以直通地球上的任何地方]]></summary></entry><entry><title type="html">线性代数读书笔记：可逆矩阵定理大串烧</title><link href="https://yuhao-yang-cy.github.io//blog/2025/inverse-matrix-theorem/" rel="alternate" type="text/html" title="线性代数读书笔记：可逆矩阵定理大串烧"/><published>2025-01-16T22:29:00+00:00</published><updated>2025-01-16T22:29:00+00:00</updated><id>https://yuhao-yang-cy.github.io//blog/2025/inverse-matrix-theorem</id><content type="html" xml:base="https://yuhao-yang-cy.github.io//blog/2025/inverse-matrix-theorem/"><![CDATA[<p>最近三天打鱼两天晒网地翻（hui2）完（lu2）了 David C. Lay 撰写的基础线性代数教材 <a href="/books/2025_linear_algebra/">Linear Algebra and Its Application</a>，一本抽象程度不高、但非常侧重应用的工科数学教材，读完学到了不少自己不曾了解过的美妙概念和神奇应用。</p> <p>这篇打算来写（shui3）一写（shui3）线性代数中最美妙的定理之一——可逆矩阵定理（inverse matrix theorem）。这条定理包含了判定一个矩阵是否可逆的一大长串等价命题。我写这篇公众号文章的原因之一是我在重读线性代数时，我发现对靠后的这些命题已经没有很深刻的印象了，所以想着就不妨逐字地在笔记里敲一遍，好歹让这些神奇的定理从大脑皮层再度划过。而另一个原因，就是终于可以把这张收藏已久的梗图拿出来镇场了。</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/inverse_matrices-480.webp 480w,/assets/img/inverse_matrices-800.webp 800w,/assets/img/inverse_matrices-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/inverse_matrices.jpg" class="img-natural rounded z-depth-1" width="100%" height="auto" title="Inverse Matrix Theorem" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 盗图来自网络 No Rights Reserved </div> <p>显然，打穿这条定理中的所有等价命题需要对线性代数这门学科不断深入的理解。我很喜欢 David Lay 在编写这本教材时将帷幕一点一点揭开的写法，过一两个章节，就适时地在等价命题的列表中添上几个新的条目。开篇从很基本的线性方程组切入，然后在提出并解决新问题的过程中，循序渐进地引入线性代数中的一系列核心概念。每隔一两个章节，读者就会发现自己从一个山头跑到了另一个山头，从一个新的视角来看待同一个问题。通过可逆矩阵定理，那些乍看起来似乎互不相关的数学概念被编织成了一套和谐的圆舞曲：逆矩阵的存在性，线性方程组的解的唯一性，相应的线性变化的双射性，列向量的线性无关性，行列式非零，几个子空间之间的联系，等等，都被紧密地交织在了一起。</p> <p>下面也不多废话了，给出可逆矩阵定理的完整表述。这是一篇纯纯的抄书笔记，因为我读的是英文原版，也懒得翻译，索性下面就违和地都用英语水了吧。。。</p> <p>For a given $n \times n$ square matrix $A$, then the following statements are all equivalent:</p> <ul> <li>$A$ is invertible.</li> <li>$A$ is row equivalent to the $n \times n$ identity matrix $I_n$.</li> <li>$A$ has $n$ pivot positions.</li> <li>The equation $A \mathbf{x} = \mathbf{0}$ has only the trivial solution $\mathbf{x} = \mathbf{0}$.</li> <li>For each vector $\mathbf{b} \in \mathbb{R}^n$, the equation $A \mathbf{x} = \mathbf{b}$ has a unique solution.</li> <li>The columns of $A$​ form a linearly independent set.</li> <li>The columns of $A$ span $\mathbb{R}^n$.</li> <li>The linear transformation $\mathbf{x} \mapsto A \mathbf{x}$​​ is one-to-one.</li> <li>The linear transformation $\mathbf{x} \mapsto A \mathbf{x}$ maps $\mathbb{R}^n$ onto $\mathbb{R}^n$.</li> <li>There exists an $n\times n$ matrix $C$ such that $CA = I_n$.</li> <li>There exists an $n \times n$ matrix $D$ such that $AD = I_n$.</li> <li>The transpose matrix $A^T$ is invertible.</li> <li>The columns of $A$ form a basis of $\mathbb{R}^n$​.</li> <li>The column space of $A$ is equal to $\mathbb{R}^n$, i.e., $\text{Col } A = \mathbb{R}^n$.</li> <li>The dimension of the column space of $A$ is $n$, i.e., $\dim \text{Col } A = n$.</li> <li>The rank of $A$ is n, i.e., $\text{rank } A = n $.</li> <li>The null space of $A$ only contains the zero vector, i.e., $\text{Nul } A = { \mathbf{0} }$.</li> <li>The dimension of the null space of $A$ is 0, i.e., $\dim \text{Nul } A = 0$​.</li> <li>The determinant of $A$ is non-zero, i.e., $\det A \neq 0$.</li> <li>The number $0$ is not an eigenvalue of $A$.</li> <li>The orthogonal complement of the column space of $A$ only contains the zero vector, i.e., $(\text{Col } A)^\perp = { \mathbf{0} }$​.</li> <li>The orthogonal complement of the null space of $A$ is equal to $\mathbb{R}^n$, i.e., $(\text{Nul } A)^\perp = \mathbb{R}^n$​.</li> <li>The row space of $A$ is equal to $\mathbb{R}^n$, i.e., $\text{Row } A = \mathbb{R}^n$​.</li> <li>$A$ has $n$ non-zero singular values.</li> </ul>]]></content><author><name></name></author><category term="mathematics"/><category term="linear-algebra"/><category term="mathematics"/><category term="linear-algebra"/><category term="matrix"/><summary type="html"><![CDATA[那些年我大约是没太整明白的问题之——判定一个矩阵是否可逆的一大长串等价命题]]></summary></entry></feed>